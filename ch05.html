<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en">
  <head>
    <meta charset="utf-8"/>
    <title>atlas book skeleton</title>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> </script>
    <link rel="stylesheet" type="text/css" href="theme/html/html.css"/>
  </head>
  <body data-type="book">
    <section data-type="chapter" class="pagenumrestart" id="a0000001854" data-pdf-bookmark="Chapter 5. Odds and Addends"><h1>Odds and Addends</h1><section data-type="sect1" id="a0000001856" data-pdf-bookmark="Odds"><h1>Odds</h1><p>One way to represent a probability is with a number between 0 and 1,
    but that’s not the only way. If you have ever bet on a football game or a
    horse race, you have probably encountered another representation of
    probability, called <strong>odds</strong>.<a data-type="indexterm" data-primary="odds" id="idp1392560"/></p><p>You might have heard expressions like “the odds are three to one,”
    but you might not know what they mean. The <strong>odds in
    favor</strong> of an event are the ratio of the probability it will
    occur to the probability that it will not.</p><p>So if I think my team has a 75% chance of winning, I would say that
    the odds in their favor are three to one, because the chance of winning is
    three times the chance of losing.</p><p>You can write odds in decimal form, but it is most common to write
    them as a ratio of integers. So “three to one” is written <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mn>3</mml:mn>

            <mml:mo>:</mml:mo>

            <mml:mn>1</mml:mn>
          </mml:mrow>
        </math>.</p><p>When probabilities are low, it is more common to report the
    <strong>odds against</strong> rather than the odds in
    favor. For example, if I think my horse has a 10% chance of winning, I
    would say that the odds against are <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mn>9</mml:mn>

            <mml:mo>:</mml:mo>

            <mml:mn>1</mml:mn>
          </mml:mrow>
        </math>.</p><p>Probabilities and odds are different representations of the same
    information. Given a probability, you can compute the odds like
    this:</p><pre data-type="programlisting">def Odds(p):
    return p / (1-p)</pre><p>Given the odds in favor, in decimal form, you can convert to
    probability like this:</p><pre data-type="programlisting">def Probability(o):
    return o / (o+1)</pre><p>If you represent odds with a numerator and denominator, you can
    convert to probability like this:</p><pre data-type="programlisting">def Probability2(yes, no):
    return yes / (yes + no)</pre><p>When I work with odds in my head, I find it helpful to picture
    people at the track. If 20% of them think my horse will win, then 80% of
    them don’t, so the odds in favor are <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mn>20</mml:mn>

            <mml:mo>:</mml:mo>

            <mml:mn>80</mml:mn>
          </mml:mrow>
        </math> or <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mn>1</mml:mn>

            <mml:mo>:</mml:mo>

            <mml:mn>4</mml:mn>
          </mml:mrow>
        </math>.</p><p>If the odds are <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mn>5</mml:mn>

            <mml:mo>:</mml:mo>

            <mml:mn>1</mml:mn>
          </mml:mrow>
        </math> against my horse, then five out of six people think
    she will lose, so the probability of winning is <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mn>1</mml:mn>

            <mml:mo>/</mml:mo>

            <mml:mn>6</mml:mn>
          </mml:mrow>
        </math>.<a data-type="indexterm" data-primary="horse racing" id="idp1412688"/></p></section><section data-type="sect1" id="a0000001890" data-pdf-bookmark="The odds form of Bayes’s theorem"><h1>The odds form of Bayes’s theorem</h1><p><a data-type="indexterm" data-primary="Bayes’s theorem" data-secondary="odds form" id="idp1414752"/>In <a data-type="xref" href="ch01.html#intro">Chapter 1</a> I wrote Bayes’s theorem in the
    <strong>probability form</strong>:</p><div data-type="equation"><math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mrow>
              <mml:mo>(</mml:mo>

              <mml:mi>H</mml:mi>

              <mml:mo>|</mml:mo>

              <mml:mi>D</mml:mi>

              <mml:mo>)</mml:mo>
            </mml:mrow>

            <mml:mo>=</mml:mo>

            <mml:mfrac>
              <mml:mrow>
                <mml:mi mathvariant="normal">p</mml:mi>

                <mml:mo>(</mml:mo>

                <mml:mi>H</mml:mi>

                <mml:mo>)</mml:mo>

                <mml:mspace width="3.33333pt"/>

                <mml:mi mathvariant="normal">p</mml:mi>

                <mml:mo>(</mml:mo>

                <mml:mi>D</mml:mi>

                <mml:mo>|</mml:mo>

                <mml:mi>H</mml:mi>

                <mml:mo>)</mml:mo>
              </mml:mrow>

              <mml:mrow>
                <mml:mi mathvariant="normal">p</mml:mi>

                <mml:mo>(</mml:mo>

                <mml:mi>D</mml:mi>

                <mml:mo>)</mml:mo>
              </mml:mrow>
            </mml:mfrac>
          </mml:mrow>
        </math></div><p>If we have two hypotheses, <em>A</em> and
    <em>B</em>, we can write the ratio of posterior
    probabilities like this:</p><div data-type="equation"><math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mfrac>
              <mml:mrow>
                <mml:mi mathvariant="normal">p</mml:mi>

                <mml:mo>(</mml:mo>

                <mml:mi>A</mml:mi>

                <mml:mo>|</mml:mo>

                <mml:mi>D</mml:mi>

                <mml:mo>)</mml:mo>
              </mml:mrow>

              <mml:mrow>
                <mml:mi mathvariant="normal">p</mml:mi>

                <mml:mo>(</mml:mo>

                <mml:mi>B</mml:mi>

                <mml:mo>|</mml:mo>

                <mml:mi>D</mml:mi>

                <mml:mo>)</mml:mo>
              </mml:mrow>
            </mml:mfrac>

            <mml:mo>=</mml:mo>

            <mml:mfrac>
              <mml:mrow>
                <mml:mi mathvariant="normal">p</mml:mi>

                <mml:mo>(</mml:mo>

                <mml:mi>A</mml:mi>

                <mml:mo>)</mml:mo>

                <mml:mspace width="3.33333pt"/>

                <mml:mi mathvariant="normal">p</mml:mi>

                <mml:mo>(</mml:mo>

                <mml:mi>D</mml:mi>

                <mml:mo>|</mml:mo>

                <mml:mi>A</mml:mi>

                <mml:mo>)</mml:mo>
              </mml:mrow>

              <mml:mrow>
                <mml:mi mathvariant="normal">p</mml:mi>

                <mml:mo>(</mml:mo>

                <mml:mi>B</mml:mi>

                <mml:mo>)</mml:mo>

                <mml:mspace width="3.33333pt"/>

                <mml:mi mathvariant="normal">p</mml:mi>

                <mml:mo>(</mml:mo>

                <mml:mi>D</mml:mi>

                <mml:mo>|</mml:mo>

                <mml:mi>B</mml:mi>

                <mml:mo>)</mml:mo>
              </mml:mrow>
            </mml:mfrac>
          </mml:mrow>
        </math></div><p>Notice that the normalizing constant, <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>D</mml:mi>

            <mml:mo>)</mml:mo>
          </mml:mrow>
        </math>, drops out of this equation.<a data-type="indexterm" data-primary="normalizing constant" id="idp1458400"/></p><p>If <em>A</em> and <em>B</em> are mutually exclusive and collectively
    exhaustive, that means <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>B</mml:mi>

            <mml:mo>)</mml:mo>

            <mml:mo>=</mml:mo>

            <mml:mn>1</mml:mn>

            <mml:mo>-</mml:mo>

            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>A</mml:mi>

            <mml:mo>)</mml:mo>
          </mml:mrow>
        </math>, so we can rewrite the ratio of the priors, and the
    ratio of the posteriors, as odds.</p><p>Writing <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">o</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>A</mml:mi>

            <mml:mo>)</mml:mo>
          </mml:mrow>
        </math> for odds in favor of <em>A</em>, we get:</p><div data-type="equation"><math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">o</mml:mi>

            <mml:mrow>
              <mml:mo>(</mml:mo>

              <mml:mi>A</mml:mi>

              <mml:mo>|</mml:mo>

              <mml:mi>D</mml:mi>

              <mml:mo>)</mml:mo>
            </mml:mrow>

            <mml:mo>=</mml:mo>

            <mml:mi mathvariant="normal">o</mml:mi>

            <mml:mrow>
              <mml:mo>(</mml:mo>

              <mml:mi>A</mml:mi>

              <mml:mo>)</mml:mo>
            </mml:mrow>

            <mml:mspace width="3.33333pt"/>

            <mml:mfrac>
              <mml:mrow>
                <mml:mi mathvariant="normal">p</mml:mi>

                <mml:mo>(</mml:mo>

                <mml:mi>D</mml:mi>

                <mml:mo>|</mml:mo>

                <mml:mi>A</mml:mi>

                <mml:mo>)</mml:mo>
              </mml:mrow>

              <mml:mrow>
                <mml:mi mathvariant="normal">p</mml:mi>

                <mml:mo>(</mml:mo>

                <mml:mi>D</mml:mi>

                <mml:mo>|</mml:mo>

                <mml:mi>B</mml:mi>

                <mml:mo>)</mml:mo>
              </mml:mrow>
            </mml:mfrac>
          </mml:mrow>
        </math></div><p>In words, this says that the posterior odds are the prior odds times
    the likelihood ratio. This is the <strong>odds
    form</strong> of Bayes’s theorem.</p><p>This form is most convenient for computing a Bayesian update on
    paper or in your head. For example, let’s go back to the cookie
    problem:<a data-type="indexterm" data-primary="cookie problem" id="idp1476736"/></p><blockquote><p>Suppose there are two bowls of cookies. Bowl 1 contains 30 vanilla
      cookies and 10 chocolate cookies. Bowl 2 contains 20 of each.</p><p>Now suppose you choose one of the bowls at random and, without
      looking, select a cookie at random. The cookie is vanilla. What is the
      probability that it came from Bowl 1?</p></blockquote><p>The prior probability is 50%, so the prior odds are <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mn>1</mml:mn>

            <mml:mo>:</mml:mo>

            <mml:mn>1</mml:mn>
          </mml:mrow>
        </math>, or just 1. The likelihood ratio is <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mfrac>
              <mml:mn>3</mml:mn>

              <mml:mn>4</mml:mn>
            </mml:mfrac>

            <mml:mo>/</mml:mo>

            <mml:mfrac>
              <mml:mn>1</mml:mn>

              <mml:mn>2</mml:mn>
            </mml:mfrac>
          </mml:mrow>
        </math>, or <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mn>3</mml:mn>

            <mml:mo>/</mml:mo>

            <mml:mn>2</mml:mn>
          </mml:mrow>
        </math>. So the posterior odds are <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mn>3</mml:mn>

            <mml:mo>:</mml:mo>

            <mml:mn>2</mml:mn>
          </mml:mrow>
        </math>, which corresponds to probability <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mn>3</mml:mn>

            <mml:mo>/</mml:mo>

            <mml:mn>5</mml:mn>
          </mml:mrow>
        </math>.</p></section><section data-type="sect1" id="oliver" data-pdf-bookmark="Oliver’s blood"><h1>Oliver’s blood</h1><p><a data-type="indexterm" data-primary="Oliver’s blood problem" id="idp1504608"/><a data-type="indexterm" data-primary="MacKay, David" id="idp1505216"/>Here is another problem from MacKay’s <em>Information
    Theory, Inference, and Learning Algorithms</em>:</p><blockquote><p>Two people have left traces of their own blood at the scene of a
      crime. A suspect, Oliver, is tested and found to have type ‘O’ blood.
      The blood groups of the two traces are found to be of type ‘O’ (a common
      type in the local population, having frequency 60%) and of type ‘AB’ (a
      rare type, with frequency 1%). Do these data [the traces found at the
      scene] give evidence in favor of the proposition that Oliver was one of
      the people [who left blood at the scene]?</p></blockquote><p>To answer this question, we need to think about what it means for
    data to give evidence in favor of (or against) a hypothesis. Intuitively,
    we might say that data favor a hypothesis if the hypothesis is more likely
    in light of the data than it was before.<a data-type="indexterm" data-primary="evidence" id="idp1509440"/></p><p>In the cookie problem, the prior odds are <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mn>1</mml:mn>

            <mml:mo>:</mml:mo>

            <mml:mn>1</mml:mn>
          </mml:mrow>
        </math>, or probability 50%. The posterior odds are
    <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mn>3</mml:mn>

            <mml:mo>:</mml:mo>

            <mml:mn>2</mml:mn>
          </mml:mrow>
        </math>, or probability 60%. So we could say that the vanilla
    cookie is evidence in favor of Bowl 1.</p><p>The odds form of Bayes’s theorem provides a way to make this
    intuition more precise. Again</p><div data-type="equation"><math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">o</mml:mi>

            <mml:mrow>
              <mml:mo>(</mml:mo>

              <mml:mi>A</mml:mi>

              <mml:mo>|</mml:mo>

              <mml:mi>D</mml:mi>

              <mml:mo>)</mml:mo>
            </mml:mrow>

            <mml:mo>=</mml:mo>

            <mml:mi mathvariant="normal">o</mml:mi>

            <mml:mrow>
              <mml:mo>(</mml:mo>

              <mml:mi>A</mml:mi>

              <mml:mo>)</mml:mo>
            </mml:mrow>

            <mml:mspace width="3.33333pt"/>

            <mml:mfrac>
              <mml:mrow>
                <mml:mi mathvariant="normal">p</mml:mi>

                <mml:mo>(</mml:mo>

                <mml:mi>D</mml:mi>

                <mml:mo>|</mml:mo>

                <mml:mi>A</mml:mi>

                <mml:mo>)</mml:mo>
              </mml:mrow>

              <mml:mrow>
                <mml:mi mathvariant="normal">p</mml:mi>

                <mml:mo>(</mml:mo>

                <mml:mi>D</mml:mi>

                <mml:mo>|</mml:mo>

                <mml:mi>B</mml:mi>

                <mml:mo>)</mml:mo>
              </mml:mrow>
            </mml:mfrac>
          </mml:mrow>
        </math></div><p>Or dividing through by <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">o</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>A</mml:mi>

            <mml:mo>)</mml:mo>
          </mml:mrow>
        </math>:</p><div data-type="equation"><math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mfrac>
              <mml:mrow>
                <mml:mi mathvariant="normal">o</mml:mi>

                <mml:mo>(</mml:mo>

                <mml:mi>A</mml:mi>

                <mml:mo>|</mml:mo>

                <mml:mi>D</mml:mi>

                <mml:mo>)</mml:mo>
              </mml:mrow>

              <mml:mrow>
                <mml:mi mathvariant="normal">o</mml:mi>

                <mml:mo>(</mml:mo>

                <mml:mi>A</mml:mi>

                <mml:mo>)</mml:mo>
              </mml:mrow>
            </mml:mfrac>

            <mml:mo>=</mml:mo>

            <mml:mfrac>
              <mml:mrow>
                <mml:mi mathvariant="normal">p</mml:mi>

                <mml:mo>(</mml:mo>

                <mml:mi>D</mml:mi>

                <mml:mo>|</mml:mo>

                <mml:mi>A</mml:mi>

                <mml:mo>)</mml:mo>
              </mml:mrow>

              <mml:mrow>
                <mml:mi mathvariant="normal">p</mml:mi>

                <mml:mo>(</mml:mo>

                <mml:mi>D</mml:mi>

                <mml:mo>|</mml:mo>

                <mml:mi>B</mml:mi>

                <mml:mo>)</mml:mo>
              </mml:mrow>
            </mml:mfrac>
          </mml:mrow>
        </math></div><p>The term on the left is the ratio of the posterior and prior odds.
    The term on the right is the likelihood ratio, also called the <strong>Bayes factor</strong>.<a data-type="indexterm" data-primary="likelihood ratio" id="idp1550064"/><a data-type="indexterm" data-primary="Bayes factor" id="idp1550672"/></p><p>If the Bayes factor value is greater than 1, that means that the
    data were more likely under <em>A</em> than under
    <em>B</em>. And since the odds ratio is also
    greater than 1, that means that the odds are greater, in light of the
    data, than they were before.</p><p>If the Bayes factor is less than 1, that means the data were less
    likely under <em>A</em> than under <em>B</em>, so the odds in favor of <em>A</em> go down.</p><p>Finally, if the Bayes factor is exactly 1, the data are equally
    likely under either hypothesis, so the odds do not change.</p><p>Now we can get back to the Oliver’s blood problem. If Oliver is one
    of the people who left blood at the crime scene, then he accounts for the
    ‘O’ sample, so the probability of the data is just the probability that a
    random member of the population has type ‘AB’ blood, which is 1%.</p><p>If Oliver did not leave blood at the scene, then we have two samples
    to account for. If we choose two random people from the population, what
    is the chance of finding one with type ‘O’ and one with type ‘AB’? Well,
    there are two ways it might happen: the first person we choose might have
    type ‘O’ and the second ‘AB’, or the other way around. So the total
    probability is <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mn>2</mml:mn>

            <mml:mo>(</mml:mo>

            <mml:mn>0</mml:mn>

            <mml:mo>.</mml:mo>

            <mml:mn>6</mml:mn>

            <mml:mo>)</mml:mo>

            <mml:mo>(</mml:mo>

            <mml:mn>0</mml:mn>

            <mml:mo>.</mml:mo>

            <mml:mn>01</mml:mn>

            <mml:mo>)</mml:mo>

            <mml:mo>=</mml:mo>

            <mml:mn>1</mml:mn>

            <mml:mo>.</mml:mo>

            <mml:mn>2</mml:mn>

            <mml:mo>%</mml:mo>
          </mml:mrow>
        </math>.</p><p>The likelihood of the data is slightly higher if Oliver is
    <em>not</em> one of the people who left blood at the scene, so
    the blood data is actually evidence against Oliver’s guilt.<a data-type="indexterm" data-primary="evidence" id="idp1566672"/></p><p>This example is a little contrived, but it is an example of the
    counterintuitive result that data <em>consistent</em> with a
    hypothesis are not necessarily <em>in favor of</em> the
    hypothesis.</p><p>If this result is so counterintuitive that it bothers you, this way
    of thinking might help: the data consist of a common event, type ‘O’
    blood, and a rare event, type ‘AB’ blood. If Oliver accounts for the
    common event, that leaves the rare event still unexplained. If Oliver
    doesn’t account for the ‘O’ blood, then we have two chances to find
    someone in the population with ‘AB’ blood. And that factor of two makes
    the difference.</p></section><section data-type="sect1" id="addends" data-pdf-bookmark="Addends"><h1>Addends</h1><p>The fundamental operation of Bayesian statistics is <code>Update</code>, which takes a prior distribution and a
    set of data, and produces a posterior distribution. But solving real
    problems usually involves a number of other operations, including scaling,
    addition and other arithmetic operations, max and min, and
    mixtures.<a data-type="indexterm" data-primary="distribution" data-secondary="operations" id="idp1566928"/></p><p>This chapter presents addition and max; I will present other
    operations as we need them.</p><p>The first example is based on
    <em>Dungeons&amp;Dragons</em>,
    a role-playing game where the results of players’ decisions are usually
    determined by rolling dice. In fact, before game play starts, players
    generate each attribute of their characters—strength, intelligence,
    wisdom, dexterity, constitution, and charisma—by rolling three 6-sided
    dice and adding them up.<a data-type="indexterm" data-primary="Dungeons and Dragons" id="idp1569744"/></p><p>So you might be curious to know the distribution of this sum. There
    are two ways you might compute it:<a data-type="indexterm" data-primary="simulation" id="idp1577872"/><a data-type="indexterm" data-primary="enumeration" id="idp1570992"/></p><dl><dt>Simulation:</dt><dd><p>Given a Pmf that represents the distribution for a single die,
          you can draw random samples, add them up, and accumulate the
          distribution of simulated sums.</p></dd><dt>Enumeration:</dt><dd><p>Given two Pmfs, you can enumerate all possible pairs of values
          and compute the distribution of the sums.</p></dd></dl><p><code>thinkbayes</code> provides
    functions for both. Here’s an example of the first approach. First, I’ll
    define a class to represent a single die as a Pmf:</p><pre data-type="programlisting">class Die(thinkbayes.Pmf):

    def __init__(self, sides):
        thinkbayes.Pmf.__init__(self)
        for x in xrange(1, sides+1):
            self.Set(x, 1)
        self.Normalize()</pre><p>Now I can create a 6-sided die:</p><pre data-type="programlisting">d6 = Die(6)</pre><p>And use <code>thinkbayes.SampleSum</code> to generate a sample of 1000
    rolls.</p><pre data-type="programlisting">dice = [d6] * 3
three = thinkbayes.SampleSum(dice, 1000)</pre><p><code>SampleSum</code> takes list
    of distributions (either Pmf or Cdf objects) and the sample size, <code>n</code>. It generates <code>n</code> random sums and returns their distribution as
    a Pmf object.</p><pre data-type="programlisting">def SampleSum(dists, n):
    pmf = MakePmfFromList(RandomSum(dists) for i in xrange(n))
    return pmf</pre><p><code>SampleSum</code> uses
    <code>RandomSum</code>, also in
    <code>thinkbayes.py</code>:</p><pre data-type="programlisting">def RandomSum(dists):
    total = sum(dist.Random() for dist in dists)
    return total</pre><p><code>RandomSum</code> invokes <code>Random</code> on each distribution and adds up the
    results.</p><p>The drawback of simulation is that the result is only approximately
    correct. As <code>n</code> gets larger,
    it gets more accurate, but of course the run time increases as
    well.</p><p>The other approach is to enumerate all pairs of values and compute
    the sum and probability of each pair. This is implemented in <code>Pmf.__add__</code>:</p><pre data-type="programlisting"># class Pmf

    def __add__(self, other):
        pmf = Pmf()
        for v1, p1 in self.Items():
            for v2, p2 in other.Items():
                pmf.Incr(v1+v2, p1*p2)
        return pmf</pre><p><code>self</code> is a Pmf, of course;
    <code>other</code> can be a Pmf or anything else
    that provides <code>Items</code>. The result is a
    new Pmf. The time to run <code>__add__</code> depends on the number of items in <code>self</code> and other; it is proportional to <code>len(self) * len(other)</code>.</p><p>And here’s how it’s used:</p><pre data-type="programlisting">    three_exact = d6 + d6 + d6</pre><p>When you apply the <code>+</code> operator to
    a Pmf, Python invokes <code>__add__</code>. In this example, <code>__add__</code> is invoked twice.</p><p><a data-type="xref" href="#fig.dungeons1">Figure 5-1</a> shows an approximate result
    generated by simulation and the exact result computed by
    enumeration.</p><figure id="fig.dungeons1" style="float: none"><img src="images/thba_0501.png"/><figcaption>Approximate and exact distributions for the sum of three 6-sided
      dice.</figcaption></figure><p><code>Pmf.__add__</code> is based
    on the assumption that the random selections from each Pmf are
    independent. In the example of rolling several dice, this assumption is
    pretty good. In other cases, we would have to extend this method to use
    conditional probabilities.<a data-type="indexterm" data-primary="independence" id="idp1597696"/></p><p>The code from this section is available from <a href="http://thinkbayes.com/dungeons.py" class="orm:hideurl">http://thinkbayes.com/dungeons.py</a>.
    For more information see <a data-type="xref" href="preface01.html#download">“Working with the code”</a>.</p></section><section data-type="sect1" id="a0000002139" data-pdf-bookmark="Maxima"><h1>Maxima</h1><p>When you generate a
    <em>Dungeons&amp;Dragons</em>
    character, you are particularly interested in the character’s best
    attributes, so you might like to know the distribution of the maximum
    attribute.</p><p>There are three ways to compute the distribution of a
    maximum:<a data-type="indexterm" data-primary="maximum" id="idp1603712"/><a data-type="indexterm" data-primary="simulation" id="idp1604320"/><a data-type="indexterm" data-primary="enumeration" id="idp1604992"/><a data-type="indexterm" data-primary="exponentiation" id="idp1605840"/></p><dl><dt>Simulation:</dt><dd><p>Given a Pmf that represents the distribution for a single
          selection, you can generate random samples, find the maximum, and
          accumulate the distribution of simulated maxima.</p></dd><dt>Enumeration:</dt><dd><p>Given two Pmfs, you can enumerate all possible pairs of values
          and compute the distribution of the maximum.</p></dd><dt>Exponentiation:</dt><dd><p>If we convert a Pmf to a Cdf, there is a simple and efficient
          algorithm for finding the Cdf of the maximum.</p></dd></dl><p>The code to simulate maxima is almost identical to the code for
    simulating sums:</p><pre data-type="programlisting">def RandomMax(dists):
    total = max(dist.Random() for dist in dists)
    return total

def SampleMax(dists, n):
    pmf = MakePmfFromList(RandomMax(dists) for i in xrange(n))
    return pmf</pre><p>All I did was replace “sum” with “max”. And the code for enumeration
    is almost identical, too:</p><pre data-type="programlisting">def PmfMax(pmf1, pmf2):
    res = thinkbayes.Pmf()
    for v1, p1 in pmf1.Items():
        for v2, p2 in pmf2.Items():
            res.Incr(max(v1, v2), p1*p2)
    return res</pre><p>In fact, you could generalize this function by taking the
    appropriate operator as a parameter.</p><p>The only problem with this algorithm is that if each Pmf has
    <em>m</em> values, the run time is proportional to
    <em>m<sup>2</sup></em>. And if we
    want the maximum of <code>k</code> selections, it
    takes time proportional to <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi>k</mml:mi>

            <mml:msup>
              <mml:mi>m</mml:mi>

              <mml:mn>2</mml:mn>
            </mml:msup>
          </mml:mrow>
        </math>.</p><p>If we convert the Pmfs to Cdfs, we can do the same calculation much
    faster! The key is to remember the definition of the cumulative
    distribution function:</p><div data-type="equation"><math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi>C</mml:mi>

            <mml:mi>D</mml:mi>

            <mml:mi>F</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>x</mml:mi>

            <mml:mo>)</mml:mo>

            <mml:mo>=</mml:mo>

            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>X</mml:mi>

            <mml:mo>≤</mml:mo>

            <mml:mspace width="3.33333pt"/>

            <mml:mi>x</mml:mi>

            <mml:mo>)</mml:mo>
          </mml:mrow>
        </math></div><p>where <em>X</em> is a random variable that
    means “a value chosen randomly from this distribution.” So, for example,
    <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi>C</mml:mi>

            <mml:mi>D</mml:mi>

            <mml:mi>F</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mn>5</mml:mn>

            <mml:mo>)</mml:mo>
          </mml:mrow>
        </math> is the probability that a value from this distribution
    is less than or equal to 5.</p><p>If I draw <em>X</em> from <em>CDF<sub>1</sub></em> and <em>Y</em> from <em>CDF<sub>2</sub></em>, and compute the
    maximum <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi>Z</mml:mi>

            <mml:mo>=</mml:mo>

            <mml:mi>m</mml:mi>

            <mml:mi>a</mml:mi>

            <mml:mi>x</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>X</mml:mi>

            <mml:mo>,</mml:mo>

            <mml:mi>Y</mml:mi>

            <mml:mo>)</mml:mo>
          </mml:mrow>
        </math>, what is the chance that <em>Z</em> is less than or equal to 5? Well, in that case
    both <em>X</em> and <em>Y</em> must be less than or equal to 5.</p><p><a data-type="indexterm" data-primary="independence" id="idp1639456"/>If the selections of <em>X</em> and
    <em>Y</em> are independent,</p><div data-type="equation"><math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi>C</mml:mi>

            <mml:mi>D</mml:mi>

            <mml:msub>
              <mml:mi>F</mml:mi>

              <mml:mn>3</mml:mn>
            </mml:msub>

            <mml:mrow>
              <mml:mo>(</mml:mo>

              <mml:mn>5</mml:mn>

              <mml:mo>)</mml:mo>
            </mml:mrow>

            <mml:mo>=</mml:mo>

            <mml:mi>C</mml:mi>

            <mml:mi>D</mml:mi>

            <mml:msub>
              <mml:mi>F</mml:mi>

              <mml:mn>1</mml:mn>
            </mml:msub>

            <mml:mrow>
              <mml:mo>(</mml:mo>

              <mml:mn>5</mml:mn>

              <mml:mo>)</mml:mo>
            </mml:mrow>

            <mml:mi>C</mml:mi>

            <mml:mi>D</mml:mi>

            <mml:msub>
              <mml:mi>F</mml:mi>

              <mml:mn>2</mml:mn>
            </mml:msub>

            <mml:mrow>
              <mml:mo>(</mml:mo>

              <mml:mn>5</mml:mn>

              <mml:mo>)</mml:mo>
            </mml:mrow>
          </mml:mrow>
        </math></div><p>where <em>CDF<sub>3</sub></em>
    is the distribution of <em>Z</em>. I chose the
    value 5 because I think it makes the formulas easy to read, but we can
    generalize for any value of <em>z</em>:</p><div data-type="equation"><math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi>C</mml:mi>

            <mml:mi>D</mml:mi>

            <mml:msub>
              <mml:mi>F</mml:mi>

              <mml:mn>3</mml:mn>
            </mml:msub>

            <mml:mrow>
              <mml:mo>(</mml:mo>

              <mml:mi>z</mml:mi>

              <mml:mo>)</mml:mo>
            </mml:mrow>

            <mml:mo>=</mml:mo>

            <mml:mi>C</mml:mi>

            <mml:mi>D</mml:mi>

            <mml:msub>
              <mml:mi>F</mml:mi>

              <mml:mn>1</mml:mn>
            </mml:msub>

            <mml:mrow>
              <mml:mo>(</mml:mo>

              <mml:mi>z</mml:mi>

              <mml:mo>)</mml:mo>
            </mml:mrow>

            <mml:mi>C</mml:mi>

            <mml:mi>D</mml:mi>

            <mml:msub>
              <mml:mi>F</mml:mi>

              <mml:mn>2</mml:mn>
            </mml:msub>

            <mml:mrow>
              <mml:mo>(</mml:mo>

              <mml:mi>z</mml:mi>

              <mml:mo>)</mml:mo>
            </mml:mrow>
          </mml:mrow>
        </math></div><p>In the special case where we draw <em>k</em>
    values from the same distribution,</p><div data-type="equation"><math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi>C</mml:mi>

            <mml:mi>D</mml:mi>

            <mml:msub>
              <mml:mi>F</mml:mi>

              <mml:mi>k</mml:mi>
            </mml:msub>

            <mml:mrow>
              <mml:mo>(</mml:mo>

              <mml:mi>z</mml:mi>

              <mml:mo>)</mml:mo>
            </mml:mrow>

            <mml:mo>=</mml:mo>

            <mml:mi>C</mml:mi>

            <mml:mi>D</mml:mi>

            <mml:msub>
              <mml:mi>F</mml:mi>

              <mml:mn>1</mml:mn>
            </mml:msub>

            <mml:msup>
              <mml:mrow>
                <mml:mo>(</mml:mo>

                <mml:mi>z</mml:mi>

                <mml:mo>)</mml:mo>
              </mml:mrow>

              <mml:mi>k</mml:mi>
            </mml:msup>
          </mml:mrow>
        </math></div><p>So to find the distribution of the maximum of <em>k</em> values, we can enumerate the probabilities in the
    given Cdf and raise them to the <em>k</em>th
    power. <code>Cdf</code> provides a
    method that does just that:</p><pre data-type="programlisting"># class Cdf

    def Max(self, k):
        cdf = self.Copy()
        cdf.ps = [p**k for p in cdf.ps]
        return cdf</pre><p><code>Max</code> takes the number
    of selections, <code>k</code>, and returns a new Cdf
    that represents the distribution of the maximum of <code>k</code> selections. The run time for this method is
    proportional to <em>m</em>, the number of items in
    the Cdf.</p><p><code>Pmf.Max</code> does the
    same thing for Pmfs. It has to do a little more work to convert the Pmf to
    a Cdf, so the run time is proportional to <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi>m</mml:mi>

            <mml:mo form="prefix">log</mml:mo>

            <mml:mi>m</mml:mi>
          </mml:mrow>
        </math>, but that’s still better than quadratic.</p><p>Finally, here’s an example that computes the distribution of a
    character’s best attribute:</p><pre data-type="programlisting">    best_attr_cdf = three_exact.Max(6)
    best_attr_pmf = best_attr_cdf.MakePmf()</pre><p>Where <code>three_exact</code> is
    defined in the previous section. If we print the results, we see that the
    chance of generating a character with at least one attribute of 18 is
    about 3%. <a data-type="xref" href="#fig.dungeons2">Figure 5-2</a> shows the distribution.</p><figure id="fig.dungeons2" style="float: none"><img src="images/thba_0502.png"/><figcaption>Distribution of the maximum of six rolls of three dice.</figcaption></figure></section><section data-type="sect1" id="mixture" data-pdf-bookmark="Mixtures"><h1>Mixtures</h1><p>Let’s do one more example from
    <em>Dungeons&amp;Dragons</em>.
    Suppose I have a box of dice with the following inventory:</p><pre data-type="programlisting">5   4-sided dice
4   6-sided dice
3   8-sided dice
2  12-sided dice
1  20-sided die</pre><p>I choose a die from the box and roll it. What is the distribution of
    the outcome?</p><p>If you know which die it is, the answer is easy. A die with <code>n</code> sides yields a uniform distribution from 1 to
    <code>n</code>, including both.<a data-type="indexterm" data-primary="uniform distribution" id="idp1692816"/></p><p>But if we don’t know which die it is, the resulting distribution is
    a <strong>mixture</strong> of uniform distributions with
    different bounds. In general, this kind of mixture does not fit any simple
    mathematical model, but it is straightforward to compute the distribution
    in the form of a PMF.<a data-type="indexterm" data-primary="mixture" id="idp1699520"/></p><p>As always, one option is to simulate the scenario, generate a random
    sample, and compute the PMF of the sample. This approach is simple and it
    generates an approximate solution quickly. But if we want an exact
    solution, we need a different approach.<a data-type="indexterm" data-primary="simulation" id="idp1700384"/></p><p>Let’s start with a simple version of the problem where there are
    only two dice, one with 6 sides and one with 8. We can make a Pmf to
    represent each die:</p><pre data-type="programlisting">    d6 = Die(6)
    d8 = Die(8)</pre><p>Then we create a Pmf to represent the mixture:</p><pre data-type="programlisting">    mix = thinkbayes.Pmf()
    for die in [d6, d8]:
        for outcome, prob in die.Items():
            mix.Incr(outcome, prob)
    mix.Normalize()</pre><p>The first loop enumerates the dice; the second enumerates the
    outcomes and their probabilities. Inside the loop, <code>Pmf.Incr</code> adds up the contributions from the two
    distributions.</p><p>This code assumes that the two dice are equally likely. More
    generally, we need to know the probability of each die so we can weight
    the outcomes accordingly.</p><p>First we create a Pmf that maps from each die to the probability it
    is selected:</p><pre data-type="programlisting">    pmf_dice = thinkbayes.Pmf()
    pmf_dice.Set(Die(4), 2)
    pmf_dice.Set(Die(6), 3)
    pmf_dice.Set(Die(8), 2)
    pmf_dice.Set(Die(12), 1)
    pmf_dice.Set(Die(20), 1)
    pmf_dice.Normalize()</pre><p>Next we need a more general version of the mixture algorithm:</p><pre data-type="programlisting">    mix = thinkbayes.Pmf()
    for die, weight in pmf_dice.Items():
        for outcome, prob in die.Items():
            mix.Incr(outcome, weight*prob)</pre><p>Now each die has a weight associated with it (which makes it a
    weighted die, I suppose). When we add each outcome to the mixture, its
    probability is multiplied by <code>weight</code>.</p><p><a data-type="xref" href="#fig.dungeons3">Figure 5-3</a> shows the result. As expected,
    values 1 through 4 are the most likely because any die can produce them.
    Values above 12 are unlikely because there is only one die in the box that
    can produce them (and it does so less than half the time).</p><figure id="fig.dungeons3" style="float: none"><img src="images/thba_0503.png"/><figcaption>Distribution outcome for random die from a box.</figcaption></figure><p><code>thinkbayes</code> provides a function
    named <code>MakeMixture</code> that encapsulates
    this algorithm, so we could have written:</p><pre data-type="programlisting">    mix = thinkbayes.MakeMixture(pmf_dice)</pre><p>We’ll use <code>MakeMixture</code> again in
    Chapters <a data-type="xref" href="ch07.html#prediction">Chapter 7</a> and
    <a data-type="xref" href="ch08.html#observer">Chapter 8</a>.</p></section><section data-type="sect1" id="a0000002320" data-pdf-bookmark="Discussion"><h1>Discussion</h1><p>Other than the odds form of Bayes’s theorem, this chapter is not
    specifically Bayesian. But Bayesian analysis is all about distributions,
    so it is important to understand the concept of a distribution well. From
    a computational point of view, a distribution is any data structure that
    represents a set of values (possible outcomes of a random process) and
    their probabilities.<a data-type="indexterm" data-primary="distribution" id="idp1721584"/></p><p>We have seen two representations of distributions: Pmfs and Cdfs.
    These representations are equivalent in the sense that they contain the
    same information, so you can convert from one to the other. The primary
    difference between them is performance: some operations are faster and
    easier with a Pmf; others are faster with a Cdf.<a data-type="indexterm" data-primary="Pmf" id="idp1722880"/><a data-type="indexterm" data-primary="Cdf" id="idp1722512"/></p><p>The other goal of this chapter is to introduce operations that act
    on distributions, like <code>Pmf.__add__</code>, <code>Cdf.Max</code>, and <code>thinkbayes.MakeMixture</code>. We will use these
    operations later, but I introduce them now to encourage you to think of a
    distribution as a fundamental unit of computation, not just a container
    for values and probabilities.</p></section></section>
  </body>
</html>
