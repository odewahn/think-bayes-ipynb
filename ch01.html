<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en">
  <head>
    <meta charset="utf-8"/>
    <title>atlas book skeleton</title>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> </script>
    <link rel="stylesheet" type="text/css" href="theme/html/html.css"/>
  </head>
  <body data-type="book">
    <section data-type="chapter" class="pagenumrestart" id="intro" data-pdf-bookmark="Chapter 1. Bayes’s Theorem"><h1>Bayes’s Theorem</h1><section data-type="sect1" id="a0000000235" data-pdf-bookmark="Conditional probability"><h1>Conditional probability</h1><p>The fundamental idea behind all Bayesian statistics is Bayes’s
    theorem, which is surprisingly easy to derive, provided that you
    understand conditional probability. So we’ll start with probability, then
    conditional probability, then Bayes’s theorem, and on to Bayesian
    statistics.<a data-type="indexterm" data-primary="conditional probability" id="idp454064"/><a data-type="indexterm" data-primary="probability" data-secondary="conditional" id="idp460720"/></p><p>A probability is a number between 0 and 1 (including both) that
    represents a degree of belief in a fact or prediction. The value 1
    represents certainty that a fact is true, or that a prediction will come
    true. The value 0 represents certainty that the fact is false.<a data-type="indexterm" data-primary="degree of belief" id="idp456736"/></p><p>Intermediate values represent degrees of certainty. The value 0.5,
    often written as 50%, means that a predicted outcome is as likely to
    happen as not. For example, the probability that a tossed coin lands face
    up is very close to 50%.<a data-type="indexterm" data-primary="coin toss" id="idp464448"/></p><p>A conditional probability is a probability based on some background
    information. For example, I want to know the probability that I will have
    a heart attack in the next year. According to the CDC, “Every year about
    785,000 Americans have a first coronary attack (<a href="http://www.cdc.gov/heartdisease/facts.htm" class="orm:hideurl">http://www.cdc.gov/heartdisease/facts.htm</a>).”<a data-type="indexterm" data-primary="heart attack" id="idp462208"/></p><p>The U.S. population is about 311 million, so the probability that a
    randomly chosen American will have a heart attack in the next year is
    roughly 0.3%.</p><p>But I am not a randomly chosen American. Epidemiologists have
    identified many factors that affect the risk of heart attacks; depending
    on those factors, my risk might be higher or lower than average.</p><p>I am male, 45 years old, and I have borderline high cholesterol.
    Those factors increase my chances. However, I have low blood pressure and
    I don’t smoke, and those factors decrease my chances.</p><p>Plugging everything into the online calculator at <a href="http://cvdrisk.nhlbi.nih.gov/calculator.asp" class="orm:hideurl">http://cvdrisk.nhlbi.nih.gov/calculator.asp</a>,
    I find that my risk of a heart attack in the next year is about 0.2%, less
    than the national average. That value is a conditional probability,
    because it is based on a number of factors that make up my
    “condition.”</p><p>The usual notation for conditional probability is <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>A</mml:mi>

            <mml:mo>|</mml:mo>

            <mml:mi>B</mml:mi>

            <mml:mo>)</mml:mo>
          </mml:mrow>
        </math>, which is the probability of <em>A</em> given that <em>B</em> is
    true. In this example, <em>A</em> represents the
    prediction that I will have a heart attack in the next year, and <em>B</em> is the set of conditions I listed.</p></section><section data-type="sect1" id="a0000000258" data-pdf-bookmark="Conjoint probability"><h1>Conjoint probability</h1><p><strong>Conjoint probability</strong> is a fancy way
    to say the probability that two things are true. I write <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>A</mml:mi>

            <mml:mspace width="3.33333pt"/>

            <mml:mi>and</mml:mi>

            <mml:mspace width="3.33333pt"/>

            <mml:mi>B</mml:mi>

            <mml:mo>)</mml:mo>
          </mml:mrow>
        </math> to mean the probability that <em>A</em> and <em>B</em> are both
    true.<a data-type="indexterm" data-primary="conjoint probability" id="idp479952"/><a data-type="indexterm" data-primary="probability" data-secondary="conjoint" id="idp480560"/></p><p>If you learned about probability in the context of coin tosses and
    dice, you might have learned the following formula:</p><div data-type="equation"><math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>A</mml:mi>

            <mml:mspace width="3.33333pt"/>

            <mml:mi>and</mml:mi>

            <mml:mspace width="3.33333pt"/>

            <mml:mi>B</mml:mi>

            <mml:mo>)</mml:mo>

            <mml:mo>=</mml:mo>

            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>A</mml:mi>

            <mml:mo>)</mml:mo>

            <mml:mspace width="3.33333pt"/>

            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>B</mml:mi>

            <mml:mo>)</mml:mo>

            <mml:mspace width="1.em"/>

            <mml:mspace width="1.em"/>

            <mml:mtext>WARNING:</mml:mtext>

            <mml:mspace width="4.pt"/>

            <mml:mtext>not</mml:mtext>

            <mml:mspace width="4.pt"/>

            <mml:mtext>always</mml:mtext>

            <mml:mspace width="4.pt"/>

            <mml:mtext>true</mml:mtext>
          </mml:mrow>
        </math></div><p>For example, if I toss two coins, and <em>A</em> means the first coin lands face up, and <em>B</em> means the second coin lands face up, then
    <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>A</mml:mi>

            <mml:mo>)</mml:mo>

            <mml:mo>=</mml:mo>

            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>B</mml:mi>

            <mml:mo>)</mml:mo>

            <mml:mo>=</mml:mo>

            <mml:mn>0</mml:mn>

            <mml:mo>.</mml:mo>

            <mml:mn>5</mml:mn>
          </mml:mrow>
        </math>, and sure enough, <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>A</mml:mi>

            <mml:mspace width="3.33333pt"/>

            <mml:mi>and</mml:mi>

            <mml:mspace width="3.33333pt"/>

            <mml:mi>B</mml:mi>

            <mml:mo>)</mml:mo>

            <mml:mo>=</mml:mo>

            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>A</mml:mi>

            <mml:mo>)</mml:mo>

            <mml:mspace width="3.33333pt"/>

            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>B</mml:mi>

            <mml:mo>)</mml:mo>

            <mml:mo>=</mml:mo>

            <mml:mn>0</mml:mn>

            <mml:mo>.</mml:mo>

            <mml:mn>25</mml:mn>
          </mml:mrow>
        </math>.</p><p>But this formula only works because in this case <em>A</em> and <em>B</em> are
    independent; that is, knowing the outcome of the first event does not
    change the probability of the second. Or, more formally, <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>B</mml:mi>

            <mml:mo>|</mml:mo>

            <mml:mi>A</mml:mi>

            <mml:mo>)</mml:mo>
          </mml:mrow>
        </math> = <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>B</mml:mi>

            <mml:mo>)</mml:mo>
          </mml:mrow>
        </math>.<a data-type="indexterm" data-primary="independence" id="idp526528"/><a data-type="indexterm" data-primary="dependence" id="idp527136"/></p><p>Here is a different example where the events are not independent.
    Suppose that <em>A</em> means that it rains today
    and <em>B</em> means that it rains tomorrow. If I
    know that it rained today, it is more likely that it will rain tomorrow,
    so <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>B</mml:mi>

            <mml:mo>|</mml:mo>

            <mml:mi>A</mml:mi>

            <mml:mo>)</mml:mo>

            <mml:mo>&gt;</mml:mo>

            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>B</mml:mi>

            <mml:mo>)</mml:mo>
          </mml:mrow>
        </math>.</p><p>In general, the probability of a conjunction is</p><div data-type="equation"><math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>A</mml:mi>

            <mml:mspace width="3.33333pt"/>

            <mml:mi>and</mml:mi>

            <mml:mspace width="3.33333pt"/>

            <mml:mi>B</mml:mi>

            <mml:mo>)</mml:mo>

            <mml:mo>=</mml:mo>

            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>A</mml:mi>

            <mml:mo>)</mml:mo>

            <mml:mspace width="3.33333pt"/>

            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>B</mml:mi>

            <mml:mo>|</mml:mo>

            <mml:mi>A</mml:mi>

            <mml:mo>)</mml:mo>
          </mml:mrow>
        </math></div><p>for any <em>A</em> and <em>B</em>. So if the chance of rain on any given day is
    0.5, the chance of rain on two consecutive days is not 0.25, but probably
    a bit higher.</p></section><section data-type="sect1" id="a0000000336" data-pdf-bookmark="The cookie problem"><h1>The cookie problem</h1><p>We’ll get to Bayes’s theorem soon, but I want to motivate it with an
    example called the cookie problem.<a data-type="noteref" id="idp550768-marker" href="ch01.html#idp550768"><sup>1</sup></a> Suppose there are two bowls of cookies. Bowl 1 contains 30
    vanilla cookies and 10 chocolate cookies. Bowl 2 contains 20 of
    each.<a data-type="indexterm" data-primary="Bayes’s theorem" id="idp551024"/><a data-type="indexterm" data-primary="cookie problem" id="idp551840"/></p><p>Now suppose you choose one of the bowls at random and, without
    looking, select a cookie at random. The cookie is vanilla. What is the
    probability that it came from Bowl 1?</p><p>This is a conditional probability; we want <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mtext>Bowl</mml:mtext>

            <mml:mspace width="4.pt"/>

            <mml:mtext>1</mml:mtext>

            <mml:mo>|</mml:mo>

            <mml:mtext>vanilla</mml:mtext>

            <mml:mo>)</mml:mo>
          </mml:mrow>
        </math>, but it is not obvious how to compute it. If I asked a
    different question—the probability of a vanilla cookie given Bowl 1—it
    would be easy:</p><div data-type="equation"><math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mtext>vanilla</mml:mtext>

            <mml:mo>|</mml:mo>

            <mml:mtext>Bowl</mml:mtext>

            <mml:mspace width="4.pt"/>

            <mml:mtext>1</mml:mtext>

            <mml:mo>)</mml:mo>

            <mml:mo>=</mml:mo>

            <mml:mn>3</mml:mn>

            <mml:mo>/</mml:mo>

            <mml:mn>4</mml:mn>
          </mml:mrow>
        </math></div><p>Sadly, <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>A</mml:mi>

            <mml:mo>|</mml:mo>

            <mml:mi>B</mml:mi>

            <mml:mo>)</mml:mo>
          </mml:mrow>
        </math> is <em>not</em> the same as
    <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>B</mml:mi>

            <mml:mo>|</mml:mo>

            <mml:mi>A</mml:mi>

            <mml:mo>)</mml:mo>
          </mml:mrow>
        </math>, but there is a way to get from one to the other:
    Bayes’s theorem.</p></section><section data-type="sect1" id="a0000000360" data-pdf-bookmark="Bayes’s theorem"><h1>Bayes’s theorem</h1><p>At this point we have everything we need to derive Bayes’s theorem.
    We’ll start with the observation that conjunction is commutative; that
    is</p><div data-type="equation"><math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>A</mml:mi>

            <mml:mspace width="3.33333pt"/>

            <mml:mi>and</mml:mi>

            <mml:mspace width="3.33333pt"/>

            <mml:mi>B</mml:mi>

            <mml:mo>)</mml:mo>

            <mml:mo>=</mml:mo>

            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>B</mml:mi>

            <mml:mspace width="3.33333pt"/>

            <mml:mi>and</mml:mi>

            <mml:mspace width="3.33333pt"/>

            <mml:mi>A</mml:mi>

            <mml:mo>)</mml:mo>
          </mml:mrow>
        </math></div><p>for any events <em>A</em> and <em>B</em>.<a data-type="indexterm" data-primary="Bayes’s theorem" data-secondary="derivation" id="idp588944"/><a data-type="indexterm" data-primary="conjunction" id="idp589792"/></p><p>Next, we write the probability of a conjunction:</p><div data-type="equation"><math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>A</mml:mi>

            <mml:mspace width="3.33333pt"/>

            <mml:mi>and</mml:mi>

            <mml:mspace width="3.33333pt"/>

            <mml:mi>B</mml:mi>

            <mml:mo>)</mml:mo>

            <mml:mo>=</mml:mo>

            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>A</mml:mi>

            <mml:mo>)</mml:mo>

            <mml:mspace width="3.33333pt"/>

            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>B</mml:mi>

            <mml:mo>|</mml:mo>

            <mml:mi>A</mml:mi>

            <mml:mo>)</mml:mo>
          </mml:mrow>
        </math></div><p>Since we have not said anything about what <em>A</em> and <em>B</em> mean, they
    are interchangeable. Interchanging them yields</p><div data-type="equation"><math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>B</mml:mi>

            <mml:mspace width="3.33333pt"/>

            <mml:mi>and</mml:mi>

            <mml:mspace width="3.33333pt"/>

            <mml:mi>A</mml:mi>

            <mml:mo>)</mml:mo>

            <mml:mo>=</mml:mo>

            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>B</mml:mi>

            <mml:mo>)</mml:mo>

            <mml:mspace width="3.33333pt"/>

            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>A</mml:mi>

            <mml:mo>|</mml:mo>

            <mml:mi>B</mml:mi>

            <mml:mo>)</mml:mo>
          </mml:mrow>
        </math></div><p>That’s all we need. Pulling those pieces together, we get</p><div data-type="equation"><math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>B</mml:mi>

            <mml:mo>)</mml:mo>

            <mml:mspace width="3.33333pt"/>

            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>A</mml:mi>

            <mml:mo>|</mml:mo>

            <mml:mi>B</mml:mi>

            <mml:mo>)</mml:mo>

            <mml:mo>=</mml:mo>

            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>A</mml:mi>

            <mml:mo>)</mml:mo>

            <mml:mspace width="3.33333pt"/>

            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>B</mml:mi>

            <mml:mo>|</mml:mo>

            <mml:mi>A</mml:mi>

            <mml:mo>)</mml:mo>
          </mml:mrow>
        </math></div><p>Which means there are two ways to compute the conjunction. If you
    have <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>A</mml:mi>

            <mml:mo>)</mml:mo>
          </mml:mrow>
        </math>, you multiply by the conditional probability
    <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>B</mml:mi>

            <mml:mo>|</mml:mo>

            <mml:mi>A</mml:mi>

            <mml:mo>)</mml:mo>
          </mml:mrow>
        </math>. Or you can do it the other way around; if you know
    <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>B</mml:mi>

            <mml:mo>)</mml:mo>
          </mml:mrow>
        </math>, you multiply by <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>A</mml:mi>

            <mml:mo>|</mml:mo>

            <mml:mi>B</mml:mi>

            <mml:mo>)</mml:mo>
          </mml:mrow>
        </math>. Either way you should get the same thing.</p><p>Finally we can divide through by <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>B</mml:mi>

            <mml:mo>)</mml:mo>
          </mml:mrow>
        </math>:</p><div data-type="equation"><math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mrow>
              <mml:mo>(</mml:mo>

              <mml:mi>A</mml:mi>

              <mml:mo>|</mml:mo>

              <mml:mi>B</mml:mi>

              <mml:mo>)</mml:mo>
            </mml:mrow>

            <mml:mo>=</mml:mo>

            <mml:mfrac>
              <mml:mrow>
                <mml:mi mathvariant="normal">p</mml:mi>

                <mml:mo>(</mml:mo>

                <mml:mi>A</mml:mi>

                <mml:mo>)</mml:mo>

                <mml:mspace width="3.33333pt"/>

                <mml:mi mathvariant="normal">p</mml:mi>

                <mml:mo>(</mml:mo>

                <mml:mi>B</mml:mi>

                <mml:mo>|</mml:mo>

                <mml:mi>A</mml:mi>

                <mml:mo>)</mml:mo>
              </mml:mrow>

              <mml:mrow>
                <mml:mi mathvariant="normal">p</mml:mi>

                <mml:mo>(</mml:mo>

                <mml:mi>B</mml:mi>

                <mml:mo>)</mml:mo>
              </mml:mrow>
            </mml:mfrac>
          </mml:mrow>
        </math></div><p>And that’s Bayes’s theorem! It might not look like much, but it
    turns out to be surprisingly powerful.</p><p>For example, we can use it to solve the cookie problem. I’ll write
    <em>B<sub>1</sub></em> for the
    hypothesis that the cookie came from Bowl 1 and <em>V</em> for the vanilla cookie. Plugging in Bayes’s
    theorem we get</p><div data-type="equation"><math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mrow>
              <mml:mo>(</mml:mo>

              <mml:msub>
                <mml:mi>B</mml:mi>

                <mml:mn>1</mml:mn>
              </mml:msub>

              <mml:mo>|</mml:mo>

              <mml:mi>V</mml:mi>

              <mml:mo>)</mml:mo>
            </mml:mrow>

            <mml:mo>=</mml:mo>

            <mml:mfrac>
              <mml:mrow>
                <mml:mi mathvariant="normal">p</mml:mi>

                <mml:mrow>
                  <mml:mo>(</mml:mo>

                  <mml:msub>
                    <mml:mi>B</mml:mi>

                    <mml:mn>1</mml:mn>
                  </mml:msub>

                  <mml:mo>)</mml:mo>
                </mml:mrow>

                <mml:mspace width="3.33333pt"/>

                <mml:mi mathvariant="normal">p</mml:mi>

                <mml:mrow>
                  <mml:mo>(</mml:mo>

                  <mml:mi>V</mml:mi>

                  <mml:mo>|</mml:mo>

                  <mml:msub>
                    <mml:mi>B</mml:mi>

                    <mml:mn>1</mml:mn>
                  </mml:msub>

                  <mml:mo>)</mml:mo>
                </mml:mrow>
              </mml:mrow>

              <mml:mrow>
                <mml:mi mathvariant="normal">p</mml:mi>

                <mml:mo>(</mml:mo>

                <mml:mi>V</mml:mi>

                <mml:mo>)</mml:mo>
              </mml:mrow>
            </mml:mfrac>
          </mml:mrow>
        </math></div><p>The term on the left is what we want: the probability of Bowl 1,
    given that we chose a vanilla cookie. The terms on the right are:</p><ul><li><p><math xmlns="http://www.w3.org/1998/Math/MathML">
              <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
                <mml:mi mathvariant="normal">p</mml:mi>

                <mml:mo>(</mml:mo>

                <mml:msub>
                  <mml:mi>B</mml:mi>

                  <mml:mn>1</mml:mn>
                </mml:msub>

                <mml:mo>)</mml:mo>
              </mml:mrow>
            </math>: This is the probability that we chose Bowl 1,
        unconditioned by what kind of cookie we got. Since the problem says we
        chose a bowl at random, we can assume <math xmlns="http://www.w3.org/1998/Math/MathML">
              <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
                <mml:mi mathvariant="normal">p</mml:mi>

                <mml:mo>(</mml:mo>

                <mml:msub>
                  <mml:mi>B</mml:mi>

                  <mml:mn>1</mml:mn>
                </mml:msub>

                <mml:mo>)</mml:mo>

                <mml:mo>=</mml:mo>

                <mml:mn>1</mml:mn>

                <mml:mo>/</mml:mo>

                <mml:mn>2</mml:mn>
              </mml:mrow>
            </math>.</p></li><li><p><math xmlns="http://www.w3.org/1998/Math/MathML">
              <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
                <mml:mi mathvariant="normal">p</mml:mi>

                <mml:mo>(</mml:mo>

                <mml:mi>V</mml:mi>

                <mml:mo>|</mml:mo>

                <mml:msub>
                  <mml:mi>B</mml:mi>

                  <mml:mn>1</mml:mn>
                </mml:msub>

                <mml:mo>)</mml:mo>
              </mml:mrow>
            </math>: This is the probability of getting a vanilla
        cookie from Bowl 1, which is 3/4.</p></li><li><p><math xmlns="http://www.w3.org/1998/Math/MathML">
              <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
                <mml:mi mathvariant="normal">p</mml:mi>

                <mml:mo>(</mml:mo>

                <mml:mi>V</mml:mi>

                <mml:mo>)</mml:mo>
              </mml:mrow>
            </math>: This is the probability of drawing a vanilla
        cookie from either bowl. Since we had an equal chance of choosing
        either bowl and the bowls contain the same number of cookies, we had
        the same chance of choosing any cookie. Between the two bowls there
        are 50 vanilla and 30 chocolate cookies, so <math xmlns="http://www.w3.org/1998/Math/MathML">
              <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
                <mml:mi mathvariant="normal">p</mml:mi>

                <mml:mo>(</mml:mo>

                <mml:mi>V</mml:mi>

                <mml:mo>)</mml:mo>
              </mml:mrow>
            </math> = 5/8.</p></li></ul><p>Putting it together, we have</p><div data-type="equation"><math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mrow>
              <mml:mo>(</mml:mo>

              <mml:msub>
                <mml:mi>B</mml:mi>

                <mml:mn>1</mml:mn>
              </mml:msub>

              <mml:mo>|</mml:mo>

              <mml:mi>V</mml:mi>

              <mml:mo>)</mml:mo>
            </mml:mrow>

            <mml:mo>=</mml:mo>

            <mml:mfrac>
              <mml:mrow>
                <mml:mo>(</mml:mo>

                <mml:mn>1</mml:mn>

                <mml:mo>/</mml:mo>

                <mml:mn>2</mml:mn>

                <mml:mo>)</mml:mo>

                <mml:mspace width="3.33333pt"/>

                <mml:mo>(</mml:mo>

                <mml:mn>3</mml:mn>

                <mml:mo>/</mml:mo>

                <mml:mn>4</mml:mn>

                <mml:mo>)</mml:mo>
              </mml:mrow>

              <mml:mrow>
                <mml:mn>5</mml:mn>

                <mml:mo>/</mml:mo>

                <mml:mn>8</mml:mn>
              </mml:mrow>
            </mml:mfrac>
          </mml:mrow>
        </math></div><p>which reduces to 3/5. So the vanilla cookie is evidence in favor of
    the hypothesis that we chose Bowl 1, because vanilla cookies are more
    likely to come from Bowl 1.<a data-type="indexterm" data-primary="evidence" id="idp719552"/></p><p>This example demonstrates one use of Bayes’s theorem: it provides a
    strategy to get from <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>B</mml:mi>

            <mml:mo>|</mml:mo>

            <mml:mi>A</mml:mi>

            <mml:mo>)</mml:mo>
          </mml:mrow>
        </math> to <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>A</mml:mi>

            <mml:mo>|</mml:mo>

            <mml:mi>B</mml:mi>

            <mml:mo>)</mml:mo>
          </mml:mrow>
        </math>. This strategy is useful in cases, like the cookie
    problem, where it is easier to compute the terms on the right side of
    Bayes’s theorem than the term on the left.</p></section><section data-type="sect1" id="a0000000505" data-pdf-bookmark="The diachronic interpretation"><h1>The diachronic interpretation</h1><p>There is another way to think of Bayes’s theorem: it gives us a way
    to update the probability of a hypothesis, <em>H</em>, in light of some body of data, <em>D</em>.<a data-type="indexterm" data-primary="diachronic interpretation" id="idp730032"/></p><p>This way of thinking about Bayes’s theorem is called the <strong>diachronic interpretation</strong>. “Diachronic” means that
    something is happening over time; in this case the probability of the
    hypotheses changes, over time, as we see new data.</p><p>Rewriting Bayes’s theorem with <em>H</em>
    and <em>D</em> yields:</p><div data-type="equation"><math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mrow>
              <mml:mo>(</mml:mo>

              <mml:mi>H</mml:mi>

              <mml:mo>|</mml:mo>

              <mml:mi>D</mml:mi>

              <mml:mo>)</mml:mo>
            </mml:mrow>

            <mml:mo>=</mml:mo>

            <mml:mfrac>
              <mml:mrow>
                <mml:mi mathvariant="normal">p</mml:mi>

                <mml:mo>(</mml:mo>

                <mml:mi>H</mml:mi>

                <mml:mo>)</mml:mo>

                <mml:mspace width="3.33333pt"/>

                <mml:mi mathvariant="normal">p</mml:mi>

                <mml:mo>(</mml:mo>

                <mml:mi>D</mml:mi>

                <mml:mo>|</mml:mo>

                <mml:mi>H</mml:mi>

                <mml:mo>)</mml:mo>
              </mml:mrow>

              <mml:mrow>
                <mml:mi mathvariant="normal">p</mml:mi>

                <mml:mo>(</mml:mo>

                <mml:mi>D</mml:mi>

                <mml:mo>)</mml:mo>
              </mml:mrow>
            </mml:mfrac>
          </mml:mrow>
        </math></div><p>In this interpretation, each term has a name:<a data-type="indexterm" data-primary="prior" id="idp750016"/><a data-type="indexterm" data-primary="posterior" id="idp750624"/><a data-type="indexterm" data-primary="likelihood" id="idp751232"/><a data-type="indexterm" data-primary="normalizing constant" id="idp751840"/></p><ul><li><p><math xmlns="http://www.w3.org/1998/Math/MathML">
              <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
                <mml:mi mathvariant="normal">p</mml:mi>

                <mml:mo>(</mml:mo>

                <mml:mi>H</mml:mi>

                <mml:mo>)</mml:mo>
              </mml:mrow>
            </math> is the probability of the hypothesis before we see
        the data, called the prior probability, or just <strong>prior</strong>.</p></li><li><p><math xmlns="http://www.w3.org/1998/Math/MathML">
              <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
                <mml:mi mathvariant="normal">p</mml:mi>

                <mml:mo>(</mml:mo>

                <mml:mi>H</mml:mi>

                <mml:mo>|</mml:mo>

                <mml:mi>D</mml:mi>

                <mml:mo>)</mml:mo>
              </mml:mrow>
            </math> is what we want to compute, the probability of the
        hypothesis after we see the data, called the <strong>posterior</strong>.</p></li><li><p><math xmlns="http://www.w3.org/1998/Math/MathML">
              <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
                <mml:mi mathvariant="normal">p</mml:mi>

                <mml:mo>(</mml:mo>

                <mml:mi>D</mml:mi>

                <mml:mo>|</mml:mo>

                <mml:mi>H</mml:mi>

                <mml:mo>)</mml:mo>
              </mml:mrow>
            </math> is the probability of the data under the
        hypothesis, called the <strong>likelihood</strong>.</p></li><li><p><math xmlns="http://www.w3.org/1998/Math/MathML">
              <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
                <mml:mi mathvariant="normal">p</mml:mi>

                <mml:mo>(</mml:mo>

                <mml:mi>D</mml:mi>

                <mml:mo>)</mml:mo>
              </mml:mrow>
            </math> is the probability of the data under any
        hypothesis, called the <strong>normalizing
        constant</strong>.</p></li></ul><p>Sometimes we can compute the prior based on background information.
    For example, the cookie problem specifies that we choose a bowl at random
    with equal probability.</p><p>In other cases the prior is subjective; that is, reasonable people
    might disagree, either because they use different background information
    or because they interpret the same information differently.<a data-type="indexterm" data-primary="subjective prior" id="idp769600"/></p><p>The likelihood is usually the easiest part to compute. In the cookie
    problem, if we know which bowl the cookie came from, we find the
    probability of a vanilla cookie by counting.</p><p>The normalizing constant can be tricky. It is supposed to be the
    probability of seeing the data under any hypothesis at all, but in the
    most general case it is hard to nail down what that means.</p><p>Most often we simplify things by specifying a set of hypotheses that
    are<a data-type="indexterm" data-primary="mutually exclusive" id="idp776160"/><a data-type="indexterm" data-primary="collectively exhaustive" id="idp771232"/></p><dl><dt>Mutually exclusive:</dt><dd><p>At most one hypothesis in the set can be true, and</p></dd><dt>Collectively exhaustive:</dt><dd><p>There are no other possibilities; at least one of the
          hypotheses has to be true.</p></dd></dl><p>I use the word <strong>suite</strong> for a set of
    hypotheses that has these properties.<a data-type="indexterm" data-primary="suite" id="idp777376"/></p><p>In the cookie problem, there are only two hypotheses—the cookie came
    from Bowl 1 or Bowl 2—and they are mutually exclusive and collectively
    exhaustive.</p><p>In that case we can compute <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>D</mml:mi>

            <mml:mo>)</mml:mo>
          </mml:mrow>
        </math> using the law of total probability, which says that if
    there are two exclusive ways that something might happen, you can add up
    the probabilities like this:</p><div data-type="equation"><math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mrow>
              <mml:mo>(</mml:mo>

              <mml:mi>D</mml:mi>

              <mml:mo>)</mml:mo>
            </mml:mrow>

            <mml:mo>=</mml:mo>

            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mrow>
              <mml:mo>(</mml:mo>

              <mml:msub>
                <mml:mi>B</mml:mi>

                <mml:mn>1</mml:mn>
              </mml:msub>

              <mml:mo>)</mml:mo>
            </mml:mrow>

            <mml:mspace width="3.33333pt"/>

            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mrow>
              <mml:mo>(</mml:mo>

              <mml:mi>D</mml:mi>

              <mml:mo>|</mml:mo>

              <mml:msub>
                <mml:mi>B</mml:mi>

                <mml:mn>1</mml:mn>
              </mml:msub>

              <mml:mo>)</mml:mo>
            </mml:mrow>

            <mml:mo>+</mml:mo>

            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mrow>
              <mml:mo>(</mml:mo>

              <mml:msub>
                <mml:mi>B</mml:mi>

                <mml:mn>2</mml:mn>
              </mml:msub>

              <mml:mo>)</mml:mo>
            </mml:mrow>

            <mml:mspace width="3.33333pt"/>

            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mrow>
              <mml:mo>(</mml:mo>

              <mml:mi>D</mml:mi>

              <mml:mo>|</mml:mo>

              <mml:msub>
                <mml:mi>B</mml:mi>

                <mml:mn>2</mml:mn>
              </mml:msub>

              <mml:mo>)</mml:mo>
            </mml:mrow>
          </mml:mrow>
        </math></div><p>Plugging in the values from the cookie problem, we have</p><div data-type="equation"><math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>D</mml:mi>

            <mml:mo>)</mml:mo>

            <mml:mo>=</mml:mo>

            <mml:mo>(</mml:mo>

            <mml:mn>1</mml:mn>

            <mml:mo>/</mml:mo>

            <mml:mn>2</mml:mn>

            <mml:mo>)</mml:mo>

            <mml:mspace width="3.33333pt"/>

            <mml:mo>(</mml:mo>

            <mml:mn>3</mml:mn>

            <mml:mo>/</mml:mo>

            <mml:mn>4</mml:mn>

            <mml:mo>)</mml:mo>

            <mml:mo>+</mml:mo>

            <mml:mo>(</mml:mo>

            <mml:mn>1</mml:mn>

            <mml:mo>/</mml:mo>

            <mml:mn>2</mml:mn>

            <mml:mo>)</mml:mo>

            <mml:mspace width="3.33333pt"/>

            <mml:mo>(</mml:mo>

            <mml:mn>1</mml:mn>

            <mml:mo>/</mml:mo>

            <mml:mn>2</mml:mn>

            <mml:mo>)</mml:mo>

            <mml:mo>=</mml:mo>

            <mml:mn>5</mml:mn>

            <mml:mo>/</mml:mo>

            <mml:mn>8</mml:mn>
          </mml:mrow>
        </math></div><p>which is what we computed earlier by mentally combining the two
    bowls.<a data-type="indexterm" data-primary="total probability" id="idp820656"/></p></section><section data-type="sect1" id="a0000000607" data-pdf-bookmark="The M&amp;Mproblem"><h1>The M&amp;Mproblem</h1><p>M&amp;M’s are small candy-coated chocolates that come in a variety
    of colors. Mars, Inc., which makes M&amp;M’s, changes the mixture of
    colors from time to time.<a data-type="indexterm" data-primary="M and M problem" id="idp820352"/></p><p>In 1995, they introduced blue M&amp;M’s. Before then, the color mix
    in a bag of plain M&amp;M’s was 30% Brown, 20% Yellow, 20% Red, 10% Green,
    10% Orange, 10% Tan. Afterward it was 24% Blue , 20% Green, 16% Orange,
    14% Yellow, 13% Red, 13% Brown.</p><p>Suppose a friend of mine has two bags of M&amp;M’s, and he tells me
    that one is from 1994 and one from 1996. He won’t tell me which is which,
    but he gives me one M&amp;Mfrom each bag. One is yellow
    and one is green. What is the probability that the yellow one came from
    the 1994 bag?</p><p>This problem is similar to the cookie problem, with the twist that I
    draw one sample from each bowl/bag. This problem also gives me a chance to
    demonstrate the table method, which is useful for solving problems like
    this on paper. In the next chapter we will solve them
    computationally.<a data-type="indexterm" data-primary="table method" id="idp824368"/></p><p>The first step is to enumerate the hypotheses. The bag the yellow
    M&amp;Mcame from I’ll call Bag 1; I’ll call the other
    Bag 2. So the hypotheses are:</p><ul><li><p>A: Bag 1 is from 1994, which implies that Bag 2 is from
        1996.</p></li><li><p>B: Bag 1 is from 1996 and Bag 2 from 1994.</p></li></ul><p>Now we construct a table with a row for each hypothesis and a column
    for each term in Bayes’s theorem:</p><table><tbody><tr><td/><td><p> Prior </p></td><td><p> Likelihood </p></td><td/><td><p> Posterior </p></td></tr><tr><td/><td><p> <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
                      <mml:mi mathvariant="normal">p</mml:mi>

                      <mml:mo>(</mml:mo>

                      <mml:mi>H</mml:mi>

                      <mml:mo>)</mml:mo>
                    </mml:mrow>
                  </math> </p></td><td><p> <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
                      <mml:mi mathvariant="normal">p</mml:mi>

                      <mml:mo>(</mml:mo>

                      <mml:mi>D</mml:mi>

                      <mml:mo>|</mml:mo>

                      <mml:mi>H</mml:mi>

                      <mml:mo>)</mml:mo>
                    </mml:mrow>
                  </math> </p></td><td><p> <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
                      <mml:mi mathvariant="normal">p</mml:mi>

                      <mml:mo>(</mml:mo>

                      <mml:mi>H</mml:mi>

                      <mml:mo>)</mml:mo>
                    </mml:mrow>
                  </math><math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
                      <mml:mi mathvariant="normal">p</mml:mi>

                      <mml:mo>(</mml:mo>

                      <mml:mi>D</mml:mi>

                      <mml:mo>|</mml:mo>

                      <mml:mi>H</mml:mi>

                      <mml:mo>)</mml:mo>
                    </mml:mrow>
                  </math> </p></td><td><p> <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
                      <mml:mi mathvariant="normal">p</mml:mi>

                      <mml:mo>(</mml:mo>

                      <mml:mi>H</mml:mi>

                      <mml:mo>|</mml:mo>

                      <mml:mi>D</mml:mi>

                      <mml:mo>)</mml:mo>
                    </mml:mrow>
                  </math> </p></td></tr><tr><td><p>A </p></td><td><p> 1/2 </p></td><td><p> (20)(20) </p></td><td><p> 200 </p></td><td><p> 20/27 </p></td></tr><tr><td><p>B </p></td><td><p> 1/2 </p></td><td><p> (10)(14) </p></td><td><p> 70 </p></td><td><p> 7/27 </p></td></tr></tbody></table><p>The first column has the priors. Based on the statement of the
    problem, it is reasonable to choose <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>A</mml:mi>

            <mml:mo>)</mml:mo>

            <mml:mo>=</mml:mo>

            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>B</mml:mi>

            <mml:mo>)</mml:mo>

            <mml:mo>=</mml:mo>

            <mml:mn>1</mml:mn>

            <mml:mo>/</mml:mo>

            <mml:mn>2</mml:mn>
          </mml:mrow>
        </math>.</p><p>The second column has the likelihoods, which follow from the
    information in the problem. For example, if <em>A</em> is true, the yellow M&amp;Mcame
    from the 1994 bag with probability 20%, and the green came from the 1996
    bag with probability 20%. Because the selections are independent, we get
    the conjoint probability by multiplying.<a data-type="indexterm" data-primary="independence" id="idp861968"/></p><p>The third column is just the product of the previous two. The sum of
    this column, 270, is the normalizing constant. To get the last column,
    which contains the posteriors, we divide the third column by the
    normalizing constant.</p><p>That’s it. Simple, right?</p><p>Well, you might be bothered by one detail. I write <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>D</mml:mi>

            <mml:mo>|</mml:mo>

            <mml:mi>H</mml:mi>

            <mml:mo>)</mml:mo>
          </mml:mrow>
        </math> in terms of percentages, not probabilities, which
    means it is off by a factor of 10,000. But that cancels out when we divide
    through by the normalizing constant, so it doesn’t affect the
    result.<a data-type="indexterm" data-primary="normalizing constant" id="idp868576"/></p><p>When the set of hypotheses is mutually exclusive and collectively
    exhaustive, you can multiply the likelihoods by any factor, if it is
    convenient, as long as you apply the same factor to the entire
    column.</p></section><section data-type="sect1" id="a0000000721" data-pdf-bookmark="The Monty Hall problem"><h1>The Monty Hall problem</h1><p>The Monty Hall problem might be the most contentious question in the
    history of probability. The scenario is simple, but the correct answer is
    so counterintuitive that many people just can’t accept it, and many smart
    people have embarrassed themselves not just by getting it wrong but by
    arguing the wrong side, aggressively, in public.<a data-type="indexterm" data-primary="Monty Hall problem" id="idp873616"/></p><p>Monty Hall was the original host of the game show <em>Let’s
    Make a Deal</em>. The Monty Hall problem is based on one of the
    regular games on the show. If you are on the show, here’s what
    happens:</p><ul><li><p>Monty shows you three closed doors and tells you that there is a
        prize behind each door: one prize is a car, the other two are less
        valuable prizes like peanut butter and fake finger nails. The prizes
        are arranged at random.</p></li><li><p>The object of the game is to guess which door has the car. If
        you guess right, you get to keep the car.</p></li><li><p>You pick a door, which we will call Door A. We’ll call the other
        doors B and C.</p></li><li><p>Before opening the door you chose, Monty increases the suspense
        by opening either Door B or C, whichever does not have the car. (If
        the car is actually behind Door A, Monty can safely open B or C, so he
        chooses one at random.)</p></li><li><p>Then Monty offers you the option to stick with your original
        choice or switch to the one remaining unopened door.</p></li></ul><p>The question is, should you “stick” or “switch” or does it make no
    difference?<a data-type="indexterm" data-primary="stick" id="idp875968"/><a data-type="indexterm" data-primary="switch" id="idp876480"/><a data-type="indexterm" data-primary="intuition" id="idp879792"/></p><p>Most people have the strong intuition that it makes no difference.
    There are two doors left, they reason, so the chance that the car is
    behind Door A is 50%.</p><p>But that is wrong. In fact, the chance of winning if you stick with
    Door A is only 1/3; if you switch, your chances are 2/3.</p><p>By applying Bayes’s theorem, we can break this problem into simple
    pieces, and maybe convince ourselves that the correct answer is, in fact,
    correct.</p><p>To start, we should make a careful statement of the data. In this
    case <em>D</em> consists of two parts: Monty
    chooses Door B <em>and</em> there is no car there.</p><p>Next we define three hypotheses: <em>A</em>,
    <em>B</em>, and <em>C</em>
    represent the hypothesis that the car is behind Door A, Door B, or Door C.
    Again, let’s apply the table method:</p><table><tbody><tr><td/><td><p> Prior </p></td><td><p> Likelihood </p></td><td/><td><p> Posterior </p></td></tr><tr><td/><td><p> <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
                      <mml:mi mathvariant="normal">p</mml:mi>

                      <mml:mo>(</mml:mo>

                      <mml:mi>H</mml:mi>

                      <mml:mo>)</mml:mo>
                    </mml:mrow>
                  </math> </p></td><td><p> <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
                      <mml:mi mathvariant="normal">p</mml:mi>

                      <mml:mo>(</mml:mo>

                      <mml:mi>D</mml:mi>

                      <mml:mo>|</mml:mo>

                      <mml:mi>H</mml:mi>

                      <mml:mo>)</mml:mo>
                    </mml:mrow>
                  </math> </p></td><td><p> <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
                      <mml:mi mathvariant="normal">p</mml:mi>

                      <mml:mo>(</mml:mo>

                      <mml:mi>H</mml:mi>

                      <mml:mo>)</mml:mo>
                    </mml:mrow>
                  </math><math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
                      <mml:mi mathvariant="normal">p</mml:mi>

                      <mml:mo>(</mml:mo>

                      <mml:mi>D</mml:mi>

                      <mml:mo>|</mml:mo>

                      <mml:mi>H</mml:mi>

                      <mml:mo>)</mml:mo>
                    </mml:mrow>
                  </math> </p></td><td><p> <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
                      <mml:mi mathvariant="normal">p</mml:mi>

                      <mml:mo>(</mml:mo>

                      <mml:mi>H</mml:mi>

                      <mml:mo>|</mml:mo>

                      <mml:mi>D</mml:mi>

                      <mml:mo>)</mml:mo>
                    </mml:mrow>
                  </math> </p></td></tr><tr><td><p>A </p></td><td><p> 1/3 </p></td><td><p> 1/2 </p></td><td><p> 1/6 </p></td><td><p> 1/3 </p></td></tr><tr><td><p>B </p></td><td><p> 1/3 </p></td><td><p> 0 </p></td><td><p> 0 </p></td><td><p> 0 </p></td></tr><tr><td><p>C </p></td><td><p> 1/3 </p></td><td><p> 1 </p></td><td><p> 1/3 </p></td><td><p> 2/3 </p></td></tr></tbody></table><p>Filling in the priors is easy because we are told that the prizes
    are arranged at random, which suggests that the car is equally likely to
    be behind any door.</p><p>Figuring out the likelihoods takes some thought, but with reasonable
    care we can be confident that we have it right:</p><ul><li><p>If the car is actually behind A, Monty could safely open Doors B
        or C. So the probability that he chooses B is 1/2. And since the car
        is actually behind A, the probability that the car is not behind B is
        1.</p></li><li><p>If the car is actually behind B, Monty has to open door C, so
        the probability that he opens door B is 0.</p></li><li><p>Finally, if the car is behind Door C, Monty opens B with
        probability 1 and finds no car there with probability 1.</p></li></ul><p>Now the hard part is over; the rest is just arithmetic. The sum of
    the third column is 1/2. Dividing through yields <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>A</mml:mi>

            <mml:mo>|</mml:mo>

            <mml:mi>D</mml:mi>

            <mml:mo>)</mml:mo>

            <mml:mo>=</mml:mo>

            <mml:mn>1</mml:mn>

            <mml:mo>/</mml:mo>

            <mml:mn>3</mml:mn>
          </mml:mrow>
        </math> and <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>C</mml:mi>

            <mml:mo>|</mml:mo>

            <mml:mi>D</mml:mi>

            <mml:mo>)</mml:mo>

            <mml:mo>=</mml:mo>

            <mml:mn>2</mml:mn>

            <mml:mo>/</mml:mo>

            <mml:mn>3</mml:mn>
          </mml:mrow>
        </math>. So you are better off switching.</p><p>There are many variations of the Monty Hall problem. One of the
    strengths of the Bayesian approach is that it generalizes to handle these
    variations.</p><p>For example, suppose that Monty always chooses B if he can, and only
    chooses C if he has to (because the car is behind B). In that case the
    revised table is:</p><table><tbody><tr><td/><td><p> Prior </p></td><td><p> Likelihood </p></td><td/><td><p> Posterior </p></td></tr><tr><td/><td><p> <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
                      <mml:mi mathvariant="normal">p</mml:mi>

                      <mml:mo>(</mml:mo>

                      <mml:mi>H</mml:mi>

                      <mml:mo>)</mml:mo>
                    </mml:mrow>
                  </math> </p></td><td><p> <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
                      <mml:mi mathvariant="normal">p</mml:mi>

                      <mml:mo>(</mml:mo>

                      <mml:mi>D</mml:mi>

                      <mml:mo>|</mml:mo>

                      <mml:mi>H</mml:mi>

                      <mml:mo>)</mml:mo>
                    </mml:mrow>
                  </math> </p></td><td><p> <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
                      <mml:mi mathvariant="normal">p</mml:mi>

                      <mml:mo>(</mml:mo>

                      <mml:mi>H</mml:mi>

                      <mml:mo>)</mml:mo>
                    </mml:mrow>
                  </math><math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
                      <mml:mi mathvariant="normal">p</mml:mi>

                      <mml:mo>(</mml:mo>

                      <mml:mi>D</mml:mi>

                      <mml:mo>|</mml:mo>

                      <mml:mi>H</mml:mi>

                      <mml:mo>)</mml:mo>
                    </mml:mrow>
                  </math> </p></td><td><p> <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
                      <mml:mi mathvariant="normal">p</mml:mi>

                      <mml:mo>(</mml:mo>

                      <mml:mi>H</mml:mi>

                      <mml:mo>|</mml:mo>

                      <mml:mi>D</mml:mi>

                      <mml:mo>)</mml:mo>
                    </mml:mrow>
                  </math> </p></td></tr><tr><td><p>A </p></td><td><p> 1/3 </p></td><td><p> 1 </p></td><td><p> 1/3 </p></td><td><p> 1/2 </p></td></tr><tr><td><p>B </p></td><td><p> 1/3 </p></td><td><p> 0 </p></td><td><p> 0 </p></td><td><p> 0 </p></td></tr><tr><td><p>C </p></td><td><p> 1/3 </p></td><td><p> 1 </p></td><td><p> 1/3 </p></td><td><p> 1/2 </p></td></tr></tbody></table><p>The only change is <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>D</mml:mi>

            <mml:mo>|</mml:mo>

            <mml:mi>A</mml:mi>

            <mml:mo>)</mml:mo>
          </mml:mrow>
        </math>. If the car is behind <em>A</em>, Monty can choose to open B or C. But in this
    variation he always chooses B, so <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>D</mml:mi>

            <mml:mo>|</mml:mo>

            <mml:mi>A</mml:mi>

            <mml:mo>)</mml:mo>

            <mml:mo>=</mml:mo>

            <mml:mn>1</mml:mn>
          </mml:mrow>
        </math>.</p><p>As a result, the likelihoods are the same for <em>A</em> and <em>C</em>, and the
    posteriors are the same: <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>A</mml:mi>

            <mml:mo>|</mml:mo>

            <mml:mi>D</mml:mi>

            <mml:mo>)</mml:mo>

            <mml:mo>=</mml:mo>

            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>C</mml:mi>

            <mml:mo>|</mml:mo>

            <mml:mi>D</mml:mi>

            <mml:mo>)</mml:mo>

            <mml:mo>=</mml:mo>

            <mml:mn>1</mml:mn>

            <mml:mo>/</mml:mo>

            <mml:mn>2</mml:mn>
          </mml:mrow>
        </math>. In this case, the fact that Monty chose B reveals no
    information about the location of the car, so it doesn’t matter whether
    the contestant sticks or switches.</p><p>On the other hand, if he had opened <em>C</em>, we would know <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>B</mml:mi>

            <mml:mo>|</mml:mo>

            <mml:mi>D</mml:mi>

            <mml:mo>)</mml:mo>

            <mml:mo>=</mml:mo>

            <mml:mn>1</mml:mn>
          </mml:mrow>
        </math>.</p><p>I included the Monty Hall problem in this chapter because I think it
    is fun, and because Bayes’s theorem makes the complexity of the problem a
    little more manageable. But it is not a typical use of Bayes’s theorem, so
    if you found it confusing, don’t worry!</p></section><section data-type="sect1" id="a0000000936" data-pdf-bookmark="Discussion"><h1>Discussion</h1><p>For many problems involving conditional probability, Bayes’s theorem
    provides a divide-and-conquer strategy. If <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>A</mml:mi>

            <mml:mo>|</mml:mo>

            <mml:mi>B</mml:mi>

            <mml:mo>)</mml:mo>
          </mml:mrow>
        </math> is hard to compute, or hard to measure experimentally,
    check whether it might be easier to compute the other terms in Bayes’s
    theorem, <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>B</mml:mi>

            <mml:mo>|</mml:mo>

            <mml:mi>A</mml:mi>

            <mml:mo>)</mml:mo>
          </mml:mrow>
        </math>, <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>A</mml:mi>

            <mml:mo>)</mml:mo>
          </mml:mrow>
        </math> and <math xmlns="http://www.w3.org/1998/Math/MathML">
          <mml:mrow xmlns:mml="http://www.w3.org/1998/Math/MathML">
            <mml:mi mathvariant="normal">p</mml:mi>

            <mml:mo>(</mml:mo>

            <mml:mi>B</mml:mi>

            <mml:mo>)</mml:mo>
          </mml:mrow>
        </math>.<a data-type="indexterm" data-primary="divide-and-conquer" id="idp1000912"/></p><p>If the Monty Hall problem is your idea of fun, I have collected a
    number of similar problems in an article called “All your Bayes are belong
    to us,” which you can read at <a href="http://allendowney.blogspot.com/2011/10/all-your-bayes-are-belong-to-us.html" class="orm:hideurl">http://allendowney.blogspot.com/2011/10/all-your-bayes-are-belong-to-us.html</a>.</p></section><aside data-type="footnotes"><p data-type="footnote" id="idp550768"><a href="ch01.html#idp550768-marker"><sup>1</sup></a> Based on an example from <a href="http://en.wikipedia.org/wiki/Bayes’_theorem" class="orm:hideurl">http://en.wikipedia.org/wiki/Bayes’_theorem</a>
        that is no longer there.</p></aside></section>
  </body>
</html>
