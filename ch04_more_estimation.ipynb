{
  "metadata": {
    "name": "More Estimation"
  },
  "nbformat": 3,
  "nbformat_minor": 0,
  "worksheets": [
    {
      "cells": [
        {
          "cell_type": "heading",
          "level": 1,
          "metadata": {
          },
          "source": "More Estimation"
        },
        {
          "cell_type": "heading",
          "level": 2,
          "metadata": {
          },
          "source": "The Euro problem"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "In _Information Theory, Inference, and Learning Algorithms_, David MacKay poses this problem:"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<blockquote>\n<p>A statistical statement appeared in âThe Guardianâ on Friday\n      January 4, 2002:</p>\n<blockquote><p>When spun on edge 250 times, a Belgian one-euro coin came up\n        heads 140 times and tails 110. âIt looks very suspicious to me,â said\n        Barry Blight, a statistics lecturer at the London School of Economics.\n        âIf the coin were unbiased, the chance of getting a result as extreme\n        as that would be less than 7%.â</p></blockquote>\n<p>But do these data give evidence that the coin is biased rather\n      than fair?</p>\n</blockquote>\n"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "To answer that question, weâll proceed in two steps. The first is to estimate the probability that the coin lands face up. The second is to evaluate whether the data support the hypothesis that the coin is biased."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "You can download the code in this section from [http://thinkbayes.com/euro.py](http://thinkbayes.com/euro.py). For more information see [âWorking with the codeâ](preface01.html#download)."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Any given coin has some probability, _x_, of landing heads up when spun on edge. It seems reasonable to believe that the value of _x_ depends on some physical characteristics of the coin, primarily the distribution of weight."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "If a coin is perfectly balanced, we expect _x_ to be close to 50%, but for a lopsided coin, _x_ might be substantially different. We can use Bayesâs theorem and the observed data to estimate _x_."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Letâs define 101 hypotheses, where _Hx_ is the hypothesis that the probability of heads is _x_%, for values from 0 to 100. Iâll start with a uniform prior where the probability of _Hx_ is the same for all _x_. Weâll come back later to consider other priors."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The likelihood function is relatively easy: If _Hx_ is true, the probability of heads is x/100 and the probability of tails is 1-x/100."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "class Euro(Suite):\n\n    def Likelihood(self, data, hypo):\n        x = hypo\n        if data == 'H':\n            return x/100.0\n        else:\n            return 1 - x/100.0",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Hereâs the code that makes the suite and updates it:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "    suite = Euro(xrange(0, 101))\n    dataset = 'H' * 140 + 'T' * 110\n\n    for data in dataset:\n        suite.Update(data)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The result is in FigureÂ 4-1."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<figure id=\"fig.euro1\" style=\"float: none\"><img src=\"files/images/thba_0401.png\"><figcaption>Posterior distribution for the Euro problem on a uniform\n      prior.</figcaption></figure>"
        },
        {
          "cell_type": "heading",
          "level": 2,
          "metadata": {
          },
          "source": "Summarizing the posterior"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Again, there are several ways to summarize the posterior distribution. One option is to find the most likely value in the posterior distribution. `thinkbayes` provides a function that does that:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "def MaximumLikelihood(pmf):\n    \"\"\"Returns the value with the highest probability.\"\"\"\n    prob, val = max((prob, val) for val, prob in pmf.Items())\n    return val",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "In this case the result is 56, which is also the observed percentage of heads, 140/250=0.56%. So that suggests (correctly) that the observed percentage is the maximum likelihood estimator for the population."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "We might also summarize the posterior by computing the mean and median:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "    print 'Mean', suite.Mean()\n    print 'Median', thinkbayes.Percentile(suite, 50)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The mean is 55.95; the median is 56. Finally, we can compute a credible interval:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "    print 'CI', thinkbayes.CredibleInterval(suite, 90)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The result is (51,61)."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Now, getting back to the original question, we would like to know whether the coin is fair. We observe that the posterior credible interval does not include 50%, which suggests that the coin is not fair."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "But that is not exactly the question we started with. MacKay asked, â Do these data give evidence that the coin is biased rather than fair?â To answer that question, we will have to be more precise about what it means to say that data constitute evidence for a hypothesis. And that is the subject of the next chapter."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "But before we go on, I want to address one possible source of confusion. Since we want to know whether the coin is fair, it might be tempting to ask for the probability that `x` is 50%:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "    print suite.Prob(50)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The result is 0.021, but that value is almost meaningless. The decision to evaluate 101 hypotheses was arbitrary; we could have divided the range into more or fewer pieces, and if we had, the probability for any given hypothesis would be greater or less."
        },
        {
          "cell_type": "heading",
          "level": 2,
          "metadata": {
          },
          "source": "Swamping the priors"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "We started with a uniform prior, but that might not be a good choice. I can believe that if a coin is lopsided, _x_ might deviate substantially from 50%, but it seems unlikely that the Belgian Euro coin is so imbalanced that _x_ is 10% or 90%."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "It might be more reasonable to choose a prior that gives higher probability to values of _x_ near 50% and lower probability to extreme values."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "As an example, I constructed a triangular prior, shown in FigureÂ 4-2. Hereâs the code that constructs the prior:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "def TrianglePrior():\n    suite = Euro()\n    for x in range(0, 51):\n        suite.Set(x, x)\n    for x in range(51, 101):\n        suite.Set(x, 100-x) \n    suite.Normalize()",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<figure id=\"fig.euro2\" style=\"float: none\"><img src=\"files/images/thba_0402.png\"><figcaption>Uniform and triangular priors for the Euro problem.</figcaption></figure>"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "FigureÂ 4-2 shows the result (and the uniform prior for comparison). Updating this prior with the same dataset yields the posterior distribution shown in FigureÂ 4-3. Even with substantially different priors, the posterior distributions are very similar. The medians and the credible intervals are identical; the means differ by less than 0.5%."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<figure id=\"fig.euro3\" style=\"float: True\"><img src=\"files/images/thba_0403.png\"><figcaption>Posterior distributions for the Euro problem.</figcaption></figure>"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "This is an example of **swamping the priors**: with enough data, people who start with different priors will tend to converge on the same posterior."
        },
        {
          "cell_type": "heading",
          "level": 2,
          "metadata": {
          },
          "source": "Optimization"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The code I have shown so far is meant to be easy to read, but it is not very efficient. In general, I like to develop code that is demonstrably correct, then check whether it is fast enough for my purposes. If so, there is no need to optimize. For this example, if we care about run time, there are several ways we can speed it up."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The first opportunity is to reduce the number of times we normalize the suite. In the original code, we call `Update` once for each spin."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "    dataset = 'H' * heads + 'T' * tails\n\n    for data in dataset:\n        suite.Update(data)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "And hereâs what `Update` looks like:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "    def Update(self, data):\n        for hypo in self.Values():\n            like = self.Likelihood(data, hypo)\n            self.Mult(hypo, like)\n        return self.Normalize()",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Each update iterates through the hypotheses, then calls `Normalize` , which iterates through the hypotheses again. We can save some time by doing all of the updates before normalizing."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "`Suite` provides a method called `UpdateSet` that does exactly that. Here it is:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "    def UpdateSet(self, dataset):\n        for data in dataset:\n            for hypo in self.Values():\n                like = self.Likelihood(data, hypo)\n                self.Mult(hypo, like)\n        return self.Normalize()",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "And hereâs how we can invoke it:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "    dataset = 'H' * heads + 'T' * tails\n    suite.UpdateSet(dataset)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "This optimization speeds things up, but the run time is still proportional to the amount of data. We can speed things up even more by rewriting `Likelihood` to process the entire dataset, rather than one spin at a time."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "In the original version, `data` is a string that encodes either heads or tails:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "    def Likelihood(self, data, hypo):\n        x = hypo / 100.0\n        if data == 'H':\n            return x\n        else:\n            return 1-x",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "As an alternative, we could encode the dataset as a tuple of two integers: the number of heads and tails. In that case `Likelihood` looks like this:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "    def Likelihood(self, data, hypo):\n        x = hypo / 100.0\n        heads, tails = data\n        like = x**heads * (1-x)**tails\n        return like",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "And then we can call `Update` like this:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "    heads, tails = 140, 110\n    suite.Update((heads, tails))",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Since we have replaced repeated multiplication with exponentiation, this version takes the same time for any number of spins."
        },
        {
          "cell_type": "heading",
          "level": 2,
          "metadata": {
          },
          "source": "The beta distribution"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "There is one more optimization that solves this problem even faster."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "So far we have used a Pmf object to represent a discrete set of values for `x` . Now we will use a continuous distribution, specifically the beta distribution (see [http://en.wikipedia.org/wiki/Beta\\_distribution](http://en.wikipedia.org/wiki/Beta_distribution))."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The beta distribution is defined on the interval from 0 to 1 (including both), so it is a natural choice for describing proportions and probabilities. But wait, it gets better."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "It turns out that if you do a Bayesian update with a binomial likelihood function, as we did in the previous section, the beta distribution is a **conjugate prior**. That means that if the prior distribution for `x` is a beta distribution, the posterior is also a beta distribution. But wait, it gets even better."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The shape of the beta distribution depends on two parameters, written _Î±_ and _Î²_, or `alpha` and `beta` . If the prior is a beta distribution with parameters `alpha` and `beta` , and we see data with `h` heads and `t` tails, the posterior is a beta distribution with parameters `alpha+h` and `beta+t` . In other words, we can do an update with two additions."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "So thatâs great, but it only works if we can find a beta distribution that is a good choice for a prior. Fortunately, for many realistic priors there is a beta distribution that is at least a good approximation, and for a uniform prior there is a perfect match. The beta distribution with `alpha=1` and `beta=1` is uniform from 0 to 1."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Letâs see how we can take advantage of all this. `thinkbayes.py` provides a class that represents a beta distribution:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "class Beta(object):\n\n    def __init__(self, alpha=1, beta=1):\n        self.alpha = alpha\n        self.beta = beta",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "By default `__init__` makes a uniform distribution. `Update` performs a Bayesian update:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "    def Update(self, data):\n        heads, tails = data\n        self.alpha += heads\n        self.beta += tails",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "`data` is a pair of integers representing the number of heads and tails."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "So we have yet another way to solve the Euro problem:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "    beta = thinkbayes.Beta()\n    beta.Update((140, 110))\n    print beta.Mean()",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "`Beta` provides `Mean` , which computes a simple function of `alpha` and `beta` :"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "    def Mean(self):\n        return float(self.alpha) / (self.alpha + self.beta)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "For the Euro problem the posterior mean is 56%, which is the same result we got using Pmfs."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "`Beta` also provides `EvalPdf` , which evaluates the probability density function (PDF) of the beta distribution:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "    def EvalPdf(self, x):\n        return x**(self.alpha-1) * (1-x)**(self.beta-1)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Finally, `Beta` provides `MakePmf` , which uses `EvalPdf` to generate a discrete approximation of the beta distribution."
        },
        {
          "cell_type": "heading",
          "level": 2,
          "metadata": {
          },
          "source": "Discussion"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "In this chapter we solved the same problem with two different priors and found that with a large dataset, the priors get swamped. If two people start with different prior beliefs, they generally find, as they see more data, that their posterior distributions converge. At some point the difference between their distribution is small enough that it has no practical effect."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "When this happens, it relieves some of the worry about objectivity that I discussed in the previous chapter. And for many real-world problems even stark prior beliefs can eventually be reconciled by data."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "But that is not always the case. First, remember that all Bayesian analysis is based on modeling decisions. If you and I do not choose the same model, we might interpret data differently. So even with the same data, we would compute different likelihoods, and our posterior beliefs might not converge."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Also, notice that in a Bayesian update, we multiply each prior probability by a likelihood, so if p(H) is 0, p(H|D) is also 0, regardless of _D_. In the Euro problem, if you are convinced that _x_ is less than 50%, and you assign probability 0 to all other hypotheses, no amount of data will convince you otherwise."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "This observation is the basis of **Cromwellâs rule**, which is the recommendation that you should avoid giving a prior probability of 0 to any hypothesis that is even remotely possible (see [http://en.wikipedia.org/wiki/Cromwellâs\\_rule](http://en.wikipedia.org/wiki/Cromwell%C3%A2%C2%80%C2%99s_rule))."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Cromwellâs rule is named after Oliver Cromwell, who wrote, âI beseech you, in the bowels of Christ, think it possible that you may be mistaken.â For Bayesians, this turns out to be good advice (even if itâs a little overwrought)."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<div class=\"hard-pagebreak\"></div>\n"
        },
        {
          "cell_type": "heading",
          "level": 2,
          "metadata": {
          },
          "source": "Exercises"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<div id=\"a0000001833\" class=\"exercise\" data-type=\"example\">\n<h5></h5>\n<p>Suppose that instead of observing coin tosses directly, you\n        measure the outcome using an instrument that is not always correct.\n        Specifically, suppose there is a probability <code>y</code> that an actual heads is reported as tails,\n        or actual tails reported as heads.</p>\n<p>Write a class that estimates the bias of a coin given a series\n        of outcomes and the value of <code>y</code>.</p>\n<p>How does the spread of the posterior distribution depend on\n        <code>y</code>?</p>\n</div>\n"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<div id=\"a0000001845\" class=\"exercise\" data-type=\"example\">\n<h5></h5>\n<p><a data-type=\"indexterm\" data-primary=\"Reddit\" id=\"idp1379248\"></a>This exercise is inspired by a question posted by a\n        âredditorâ named dominosci on Redditâs statistics âsubredditâ at\n        <a href=\"http://reddit.com/r/statistics\" class=\"orm:hideurl\">http://reddit.com/r/statistics</a>.</p>\n<p>Reddit is an online forum with many interest groups called\n        subreddits. Users, called redditors, post links to online content and\n        other web pages. Other redditors vote on the links, giving an âupvoteâ\n        to high-quality links and a âdownvoteâ to links that are bad or\n        irrelevant.</p>\n<p>A problem, identified by dominosci, is that some redditors are\n        more reliable than others, and Reddit does not take this into\n        account.</p>\n<p>The challenge is to devise a system so that when a redditor\n        casts a vote, the estimated quality of the link is updated in\n        accordance with the reliability of the redditor, and the estimated\n        reliability of the redditor is updated in accordance with the quality\n        of the link.</p>\n<p>One approach is to model the quality of the link as the\n        probability of garnering an upvote, and to model the reliability of\n        the redditor as the probability of correctly giving an upvote to a\n        high-quality item.</p>\n<p>Write class definitions for redditors and links and an update\n        function that updates both objects whenever a redditor casts a\n        vote.</p>\n</div>"
        }
      ],
      "metadata": {
      }
    }
  ]
}