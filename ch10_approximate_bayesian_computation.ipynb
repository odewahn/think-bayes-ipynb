{
  "metadata": {
    "name": "Approximate Bayesian Computation"
  },
  "nbformat": 3,
  "nbformat_minor": 0,
  "worksheets": [
    {
      "cells": [
        {
          "cell_type": "heading",
          "level": 1,
          "metadata": {
          },
          "source": "Approximate Bayesian Computation"
        },
        {
          "cell_type": "heading",
          "level": 2,
          "metadata": {
          },
          "source": "The Variability Hypothesis"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "I have a soft spot for crank science. Recently I visited Norumbega Tower, which is an enduring monument to the crackpot theories of Eben Norton Horsford, inventor of double-acting baking powder and fake history. But thatâs not what this chapter is about."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "This chapter is about the Variability Hypothesis, which"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<blockquote>\n<p>âoriginated in the early nineteenth century with Johann Meckel,\n      who argued that males have a greater range of ability than females,\n      especially in intelligence. In other words, he believed that most\n      geniuses and most mentally retarded people are men. Because he\n      considered males to be the âsuperior animal,â Meckel concluded that\n      femalesâ lack of variation was a sign of inferiority.â</p>\n<p>From <a href=\"http://en.wikipedia.org/wiki/Variability_hypothesis\" class=\"orm:hideurl\">http://en.wikipedia.org/wiki/Variability_hypothesis</a>.</p>\n</blockquote>\n"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "I particularly like that last part, because I suspect that if it turns out that women are actually more variable, Meckel would take that as a sign of inferiority, too. Anyway, you will not be surprised to hear that the evidence for the Variability Hypothesis is weak."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Nevertheless, it came up in my class recently when we looked at data from the CDCâs Behavioral Risk Factor Surveillance System (BRFSS), specifically the self-reported heights of adult American men and women. The dataset includes responses from 154407 men and 254722 women. Hereâs what we found:"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<ul>\n<li><p>The average height for men is 178 cm; the average height for\n        women is 163 cm. So men are taller, on average. No surprise\n        there.</p></li>\n<li><p>For men the standard deviation is 7.7 cm; for women it is 7.3\n        cm. So in absolute terms, menâs heights are more variable.</p></li>\n<li><p>But to compare variability between groups, it is more meaningful\n        to use the coefficient of variation (CV), which is the standard\n        deviation divided by the mean. It is a dimensionless measure of\n        variability relative to scale. For men CV is 0.0433; for women it is\n        0.0444.<a data-type=\"indexterm\" data-primary=\"coefficient of variation\" id=\"idp2404624\"></a></p></li>\n</ul>\n"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Thatâs very close, so we could conclude that this dataset provides weak evidence against the Variability Hypothesis. But we can use Bayesian methods to make that conclusion more precise. And answering this question gives me a chance to demonstrate some techniques for working with large datasets."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "I will proceed in a few steps:"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<ol>\n<li><p>Weâll start with the simplest implementation, but it only works\n        for datasets smaller than 1000 values.</p></li>\n<li><p>By computing probabilities under a log transform, we can scale\n        up to the full size of the dataset, but the computation gets\n        slow.</p></li>\n<li><p>Finally, we speed things up substantially with Approximate\n        Bayesian Computation, also known as ABC.</p></li>\n</ol>\n"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "You can download the code in this chapter from [http://thinkbayes.com/variability.py](http://thinkbayes.com/variability.py). For more information see [âWorking with the codeâ](preface01.html#download)."
        },
        {
          "cell_type": "heading",
          "level": 2,
          "metadata": {
          },
          "source": "Mean and standard deviation"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "In [ChapterÂ 9](ch09.html#paintball) we estimated two parameters simultaneously using a joint distribution. In this chapter we use the same method to estimate the parameters of a Gaussian distribution: the mean, `mu` , and the standard deviation, `sigma` ."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "For this problem, I define a Suite called `Height` that represents a map from each `mu, sigma` pair to its probability:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "class Height(thinkbayes.Suite, thinkbayes.Joint):\n\n    def __init__(self, mus, sigmas):\n        thinkbayes.Suite.__init__(self)\n\n        pairs = [(mu, sigma) \n                 for mu in mus\n                 for sigma in sigmas]\n\n        thinkbayes.Suite.__init__(self, pairs)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "`mus` is a sequence of possible values for `mu` ; `sigmas` is a sequence of values for `sigma` . The prior distribution is uniform over all `mu, sigma` pairs."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The likelihood function is easy. Given hypothetical values of `mu` and `sigma` , we compute the likelihood of a particular value, `x` . Thatâs what `EvalGaussianPdf` does, so all we have to do is use it:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "# class Height\n\n    def Likelihood(self, data, hypo):\n        x = data\n        mu, sigma = hypo\n        like = thinkbayes.EvalGaussianPdf(x, mu, sigma)\n        return like",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "If you have studied statistics from a mathematical perspective, you know that when you evaluate a PDF, you get a probability density. In order to get a probability, you have to integrate probability densities over some range."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "But for our purposes, we donât need a probability; we just need something proportional to the probability we want. A probability density does that job nicely."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The hardest part of this problem turns out to be choosing appropriate ranges for `mus` and `sigmas` . If the range is too small, we omit some possibilities with non-negligible probability and get the wrong answer. If the range is too big, we get the right answer, but waste computational power."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "So this is an opportunity to use classical estimation to make Bayesian techniques more efficient. Specifically, we can use classical estimators to find a likely location for `mu` and `sigma` , and use the standard errors of those estimates to choose a likely spread."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "If the true parameters of the distribution are _Î¼_ and _Ï_, and we take a sample of _n_ values, an estimator of _Î¼_ is the sample mean, `m` ."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "And an estimator of _Ï_ is the sample standard variance, `s` ."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The standard error of the estimated _Î¼_ is s/n and the standard error of the estimated _Ï_ is s/2(n-1)."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Hereâs the code to compute all that:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "def FindPriorRanges(xs, num_points, num_stderrs=3.0):\n\n    # compute m and s\n    n = len(xs)\n    m = numpy.mean(xs)\n    s = numpy.std(xs)\n\n    # compute ranges for m and s\n    stderr_m = s / math.sqrt(n)\n    mus = MakeRange(m, stderr_m)\n\n    stderr_s = s / math.sqrt(2 * (n-1))\n    sigmas = MakeRange(s, stderr_s)\n\n    return mus, sigmas",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "`xs` is the dataset. `num_points` is the desired number of values in the range. `num_stderrs` is the width of the range on each side of the estimate, in number of standard errors."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The return value is a pair of sequences, `mus` and `sigmas` ."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Hereâs `MakeRange` :"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "    def MakeRange(estimate, stderr):\n        spread = stderr * num_stderrs\n        array = numpy.linspace(estimate-spread,\n                               estimate+spread,\n                               num_points)\n        return array",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "`numpy.linspace` makes an array of equally spaced elements between `estimate-spread` and `estimate+spread` , including both."
        },
        {
          "cell_type": "heading",
          "level": 2,
          "metadata": {
          },
          "source": "Update"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Finally hereâs the code to make and update the suite:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "    mus, sigmas = FindPriorRanges(xs, num_points)\n    suite = Height(mus, sigmas)\n    suite.UpdateSet(xs)\n    print suite.MaximumLikelihood()",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "This process might seem bogus, because we use the data to choose the range of the prior distribution, and then use the data again to do the update. In general, using the same data twice is, in fact, bogus."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "But in this case it is ok. Really. We use the data to choose the range for the prior, but only to avoid computing a lot of probabilities that would have been very small anyway. With `num_stderrs=4` , the range is big enough to cover all values with non-negligible likelihood. After that, making it bigger has no effect on the results."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "In effect, the prior is uniform over all values of `mu` and `sigma` , but for computational efficiency we ignore all the values that donât matter."
        },
        {
          "cell_type": "heading",
          "level": 2,
          "metadata": {
          },
          "source": "The posterior distribution of CV"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Once we have the posterior joint distribution of `mu` and `sigma` , we can compute the distribution of CV for men and women, and then the probability that one exceeds the other."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": ""
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<div class=\"hard-pagebreak\"></div>"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "mu",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "sigma",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "def CoefVariation(suite):\n    pmf = thinkbayes.Pmf()\n    for (mu, sigma), p in suite.Items():\n        pmf.Incr(sigma/mu, p)\n    return pmf",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Then we use `thinkbayes.PmfProbGreater` to compute the probability that men are more variable."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The analysis itself is simple, but there are two more issues we have to deal with:"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<ol>\n<li><p>As the size of the dataset increases, we run into a series of\n        computational problems due to the limitations of floating-point\n        arithmetic.</p></li>\n<li><p>The dataset contains a number of extreme values that are almost\n        certainly errors. We will need to make the estimation process robust\n        in the presence of these outliers.</p></li>\n</ol>\n"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The following sections explain these problems and their solutions."
        },
        {
          "cell_type": "heading",
          "level": 2,
          "metadata": {
          },
          "source": "Underflow"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "If we select the first 100 values from the BRFSS dataset and run the analysis I just described, it runs without errors and we get posterior distributions that look reasonable."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "If we select the first 1000 values and run the program again, we get an error in `Pmf.Normalize` :"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "ValueError: total probability is zero.",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The problem is that we are using probability densities to compute likelihoods, and densities from continuous distributions tend to be small. And if you take 1000 small values and multiply them together, the result is very small. In this case it is so small it canât be represented by a floating-point number, so it gets rounded down to zero, which is called **underflow**. And if all probabilities in the distribution are 0, itâs not a distribution any more."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "A possible solution is to renormalize the Pmf after each update, or after each batch of 100. That would work, but it would be slow."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "A better alternative is to compute likelihoods under a log transform. That way, instead of multiplying small values, we can add up log likelihoods. `Pmf` provides methods `Log` , `LogUpdateSet` and `Exp` to make this process easy."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<div class=\"hard-pagebreak\"></div>\n"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "`Log` computes the log of the probabilities in a Pmf:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "# class Pmf\n\n    def Log(self):\n        m = self.MaxLike()\n        for x, p in self.d.iteritems():\n            if p:\n                self.Set(x, math.log(p/m))\n            else:\n                self.Remove(x)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Before applying the log transform `Log` uses `MaxLike` to find `m` , the highest probability in the Pmf. It divide all probabilities by `m` , so the highest probability gets normalized to 1, which yields a log of 0. The other log probabilities are all negative. If there are any values in the Pmf with probability 0, they are removed."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "While the Pmf is under a log transform, we canât use `Update` , `UpdateSet` , or `Normalize` . The result would be nonsensical; if you try, Pmf raises an exception. Instead, we have to use `LogUpdate` and `LogUpdateSet` ."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Hereâs the implementation of `LogUpdateSet` :"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "# class Suite\n\n    def LogUpdateSet(self, dataset):\n        for data in dataset:\n            self.LogUpdate(data)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "`LogUpdateSet` loops through the data and calls `LogUpdate` :"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "# class Suite\n\n    def LogUpdate(self, data):\n        for hypo in self.Values():\n            like = self.LogLikelihood(data, hypo)\n            self.Incr(hypo, like)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "`LogUpdate` is just like `Update` except that it calls `LogLikelihood` instead of `Likelihood` , and `Incr` instead of `Mult` ."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Using log-likelihoods avoids the problem with underflow, but while the Pmf is under the log transform, thereâs not much we can do with it. We have to use `Exp` to invert the transform:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "# class Pmf\n\n    def Exp(self):\n        m = self.MaxLike()\n        for x, p in self.d.iteritems():\n            self.Set(x, math.exp(p-m))",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "If the log-likelihoods are large negative numbers, the resulting likelihoods might underflow. So `Exp` finds the maximum log-likelihood, `m` , and shifts all the likelihoods up by `m` . The resulting distribution has a maximum likelihood of 1. This process inverts the log transform with minimal loss of precision."
        },
        {
          "cell_type": "heading",
          "level": 2,
          "metadata": {
          },
          "source": "Log-likelihood"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Now all we need is `LogLikelihood` ."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "# class Height\n\n    def LogLikelihood(self, data, hypo):\n        x = data\n        mu, sigma = hypo\n        loglike = scipy.stats.norm.logpdf(x, mu, sigma)\n        return loglike",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "`norm.logpdf` computes the log-likelihood of the Gaussian PDF."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Hereâs what the whole update process looks like:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "    suite.Log()\n    suite.LogUpdateSet(xs)\n    suite.Exp()\n    suite.Normalize()",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "To review, `Log` puts the suite under a log transform. `LogUpdateSet` calls `LogUpdate` , which calls `LogLikelihood` . `LogUpdate` uses `Pmf.Incr` , because adding a log-likelihood is the same as multiplying by a likelihood."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "After the update, the log-likelihoods are large negative numbers, so `Exp` shifts them up before inverting the transform, which is how we avoid underflow."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Once the suite is transformed back, the probabilities are âlinearâ again, which means ânot logarithmicâ, so we can use `Normalize` again."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Using this algorithm, we can process the entire dataset without underflow, but it is still slow. On my computer it might take an hour. We can do better."
        },
        {
          "cell_type": "heading",
          "level": 2,
          "metadata": {
          },
          "source": "A little optimization"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "This section uses math and computational optimization to speed things up by a factor of 100. But the following section presents an algorithm that is even faster. So if you want to get right to the good stuff, feel free to skip this section."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "`Suite.LogUpdateSet` calls `LogUpdate` once for each data point. We can speed it up by computing the log-likelihood of the entire dataset at once."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Weâll start with the Gaussian PDF:"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<div data-type=\"equation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mfrac><mn>1</mn><mrow><mi>Ï</mi><msqrt><mrow><mn>2</mn><mi>Ï</mi></mrow></msqrt></mrow></mfrac><mo form=\"prefix\">exp</mo><mfenced close=\"]\" open=\"[\" separators=\"\"><mo>-</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><msup><mfenced close=\")\" open=\"(\" separators=\"\"><mfrac><mrow><mi>x</mi><mo>-</mo><mi>Î¼</mi></mrow><mi>Ï</mi></mfrac></mfenced><mn>2</mn></msup></mfenced></mrow></math></div>\n"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "and compute the log (dropping the constant term):"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<div data-type=\"equation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mo>-</mo><mo form=\"prefix\">log</mo><mi>Ï</mi><mo>-</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><msup><mfenced close=\")\" open=\"(\" separators=\"\"><mfrac><mrow><mi>x</mi><mo>-</mo><mi>Î¼</mi></mrow><mi>Ï</mi></mfrac></mfenced><mn>2</mn></msup></mrow></math></div>\n"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Given a sequence of values, _xi_, the total log-likelihood is"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<div data-type=\"equation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><munder><mo>â</mo><mi>i</mi></munder><mo>-</mo><mo form=\"prefix\">log</mo><mi>Ï</mi><mo>-</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><msup><mfenced close=\")\" open=\"(\" separators=\"\"><mfrac><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>-</mo><mi>Î¼</mi></mrow><mi>Ï</mi></mfrac></mfenced><mn>2</mn></msup></mrow></math></div>\n"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Pulling out the terms that donât depend on _i_, we get"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<div data-type=\"equation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mo>-</mo><mi>n</mi><mo form=\"prefix\">log</mo><mi>Ï</mi><mo>-</mo><mfrac><mn>1</mn><mrow><mn>2</mn><msup><mi>Ï</mi><mn>2</mn></msup></mrow></mfrac><munder><mo>â</mo><mi>i</mi></munder><msup><mrow><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>-</mo><mi>Î¼</mi><mo>)</mo></mrow><mn>2</mn></msup></mrow></math></div>\n"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "which we can translate into Python:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "# class Height\n\n    def LogUpdateSetFast(self, data):\n        xs = tuple(data)\n        n = len(xs)\n\n        for hypo in self.Values():\n            mu, sigma = hypo\n            total = Summation(xs, mu)\n            loglike = -n * math.log(sigma) - total / 2 / sigma**2\n            self.Incr(hypo, loglike)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "By itself, this would be a small improvement, but it creates an opportunity for a bigger one. Notice that the summation only depends on `mu` , not `sigma` , so we only have to compute it once for each value of `mu` ."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "To avoid recomputing, I factor out a function that computes the summation, and **memoize** it so it stores previously computed results in a dictionary (see [http://en.wikipedia.org/wiki/Memoization](http://en.wikipedia.org/wiki/Memoization)):"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "def Summation(xs, mu, cache={}):\n    try:\n        return cache[xs, mu]\n    except KeyError:\n        ds = [(x-mu)**2 for x in xs]\n        total = sum(ds)\n        cache[xs, mu] = total\n        return total",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "`cache` stores previously computed sums. The `try` statement returns a result from the cache if possible; otherwise it computes the summation, then caches and returns the result."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The only catch is that we canât use a list as a key in the cache, because it is not a hashable type. Thatâs why `LogUpdateSetFast` converts the dataset to a tuple."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "This optimization speeds up the computation by about a factor of 100, processing the entire dataset (154407 men and 254722 women) in less than a minute on my not-very-fast computer."
        },
        {
          "cell_type": "heading",
          "level": 2,
          "metadata": {
          },
          "source": "ABC"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "But maybe you donât have that kind of time. In that case, Approximate Bayesian Computation (ABC) might be the way to go. The motivation behind ABC is that the likelihood of any particular dataset is:"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<ol>\n<li><p>Very small, especially for large datasets, which is why we had\n        to use the log transform,</p></li>\n<li><p>Expensive to compute, which is why we had to do so much\n        optimization, and</p></li>\n<li><p>Not really what we want anyway.</p></li>\n</ol>\n"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "We donât really care about the likelihood of seeing the exact dataset we saw. Especially for continuous variables, we care about the likelihood of seeing any dataset like the one we saw."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "For example, in the Euro problem, we donât care about the order of the coin flips, only the total number of heads and tails. And in the locomotive problem, we donât care about which particular trains were seen, only the number of trains and the maximum of the serial numbers."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Similarly, in the BRFSS sample, we donât really want to know the probability of seeing one particular set of values (especially since there are hundreds of thousands of them). It is more relevant to ask, âIf we sample 100,000 people from a population with hypothetical values of _Î¼_ and _Ï_, what would be the chance of collecting a sample with the observed mean and variance?â"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "For samples from a Gaussian distribution, we can answer this question efficiently because we can find the distribution of the sample statistics analytically. In fact, we already did it when we computed the range of the prior."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "If you draw _n_ values from a Gaussian distribution with parameters _Î¼_ and _Ï_, and compute the sample mean, _m_, the distribution of _m_ is Gaussian with parameters _Î¼_ and Ï/n."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Similarly, the distribution of the sample standard deviation, _s_, is Gaussian with parameters _Ï_ and Ï/2(n-1)."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "We can use these sample distributions to compute the likelihood of the sample statistics, _m_ and _s_, given hypothetical values for _Î¼_ and _Ï_. Hereâs a new version of `LogUpdateSet` that does it:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "    def LogUpdateSetABC(self, data):\n        xs = data\n        n = len(xs)\n\n        # compute sample statistics\n        m = numpy.mean(xs)\n        s = numpy.std(xs)\n\n        for hypo in sorted(self.Values()):\n            mu, sigma = hypo\n\n            # compute log likelihood of m, given hypo\n            stderr_m = sigma / math.sqrt(n)\n            loglike = EvalGaussianLogPdf(m, mu, stderr_m)\n\n            #compute log likelihood of s, given hypo\n            stderr_s = sigma / math.sqrt(2 * (n-1))\n            loglike += EvalGaussianLogPdf(s, sigma, stderr_s)\n\n            self.Incr(hypo, loglike)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "On my computer this function processes the entire dataset in about a second, and the result agrees with the exact result with about 5 digits of precision."
        },
        {
          "cell_type": "heading",
          "level": 2,
          "metadata": {
          },
          "source": "Robust estimation"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "We are almost ready to look at results, but we have one more problem to deal with. There are a number of outliers in this dataset that are almost certainly errors. For example, there are three adults with reported height of 61 cm, which would place them among the shortest living adults in the world. At the other end, there are four women with reported height 229 cm, just short of the tallest women in the world."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "It is not impossible that these values are correct, but it is unlikely, which makes it hard to know how to deal with them. And we have to get it right, because these extreme values have a disproportionate effect on the estimated variability."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Because ABC is based on summary statistics, rather than the entire dataset, we can make it more robust by choosing summary statistics that are robust in the presence of outliers. For example, rather than use the sample mean and standard deviation, we could use the median and inter-quartile range (IQR), which is the difference between the 25th and 75th percentiles."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "More generally, we could compute an inter-percentile range (IPR) that spans any given fraction of the distribution, `p` :"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "def MedianIPR(xs, p):\n    cdf = thinkbayes.MakeCdfFromList(xs)\n    median = cdf.Percentile(50)\n\n    alpha = (1-p) / 2\n    ipr = cdf.Value(1-alpha) - cdf.Value(alpha)\n    return median, ipr",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "`xs` is a sequence of values. `p` is the desired range; for example, `p=0.5` yields the inter-quartile range."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "`MedianIPR` works by computing the CDF of `xs` , then extracting the median and the difference between two percentiles."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "We can convert from `ipr` to an estimate of `sigma` using the Gaussian CDF to compute the fraction of the distribution covered by a given number of standard deviations. For example, it is a well-known rule of thumb that 68% of a Gaussian distribution falls within one standard deviation of the mean, which leaves 16% in each tail. If we compute the range between the 16th and 84th percentiles, we expect the result to be `2 * sigma` . So we can estimate `sigma` by computing the 68% IPR and dividing by 2."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "More generally we could use any number of `sigmas` . `MedianS` performs the more general version of this computation:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "def MedianS(xs, num_sigmas):\n    half_p = thinkbayes.StandardGaussianCdf(num_sigmas) - 0.5\n\n    median, ipr = MedianIPR(xs, half_p * 2)\n    s = ipr / 2 / num_sigmas\n\n    return median, s",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Again, `xs` is the sequence of values; `num_sigmas` is the number of standard deviations the results should be based on. The result is `median` , which estimates _Î¼_, and `s` , which estimates _Ï_."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Finally, in `LogUpdateSetABC` we can replace the sample mean and standard deviation with `median` and `s` . And that pretty much does it."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "It might seem odd that we are using observed percentiles to estimate _Î¼_ and _Ï_, but it is an example of the flexibility of the Bayesian approach. In effect we are asking, âGiven hypothetical values for _Î¼_ and _Ï_, and a sampling process that has some chance of introducing errors, what is the likelihood of generating a given set of sample statistics?â"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "We are free to choose any sample statistics we like, up to a point: _Î¼_ and _Ï_ determine the location and spread of a distribution, so we need to choose statistics that capture those characteristics. For example, if we chose the 49th and 51st percentiles, we would get very little information about spread, so it would leave the estimate of _Ï_ relatively unconstrained by the data. All values of `sigma` would have nearly the same likelihood of producing the observed values, so the posterior distribution of `sigma` would look a lot like the prior."
        },
        {
          "cell_type": "heading",
          "level": 2,
          "metadata": {
          },
          "source": "Who is more variable?"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Finally we are ready to answer the question we started with: is the coefficient of variation greater for men than for women?"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Using ABC based on the median and IPR with `num_sigmas=1` , I computed posterior joint distributions for `mu` and `sigma` . Figures FigureÂ 10-1 and FigureÂ 10-2 show the results as a contour plot with `mu` on the x-axis, `sigma` on the y-axis, and probability on the z-axis."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<figure id=\"fig.variability1\" style=\"float: none\"><img src=\"files/images/thba_1001.png\"><figcaption>Contour plot of the posterior joint distribution of mean and\n      standard deviation of height for men in the U.S.</figcaption></figure>"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<figure id=\"fig.variability2\" style=\"float: True\"><img src=\"files/images/thba_1002.png\"><figcaption>Contour plot of the posterior joint distribution of mean and\n      standard deviation of height for women in the U.S.</figcaption></figure>"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "For each joint distribution, I computed the posterior distribution of CV. FigureÂ 10-3 shows these distributions for men and women. The mean for men is 0.0410; for women it is 0.0429. Since there is no overlap between the distributions, we conclude with near certainty that women are more variable in height than men."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<figure id=\"fig.variability3\" style=\"float: True\"><img src=\"files/images/thba_1003.png\"><figcaption>Posterior distributions of CV for men and women, based on robust\n      <span class=\"keep-together\">estimators</span>.</figcaption></figure>"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "So is that the end of the Variability Hypothesis? Sadly, no. It turns out that this result depends on the choice of the inter-percentile range. With `num_sigmas=1` , we conclude that women are more variable, but with `num_sigmas=2` we conclude with equal confidence that men are more variable."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The reason for the difference is that there are more men of short stature, and their distance from the mean is greater."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "So our evaluation of the Variability Hypothesis depends on the interpretation of âvariability.â With `num_sigmas=1` we focus on people near the mean. As we increase `num_sigmas` , we give more weight to the extremes."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "To decide which emphasis is appropriate, we would need a more precise statement of the hypothesis. As it is, the Variability Hypothesis may be too vague to evaluate."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Nevertheless, it helped me demonstrate several new ideas and, I hope you agree, it makes an interesting example."
        },
        {
          "cell_type": "heading",
          "level": 2,
          "metadata": {
          },
          "source": "Discussion"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "There are two ways you might think of ABC. One interpretation is that it is, as the name suggests, an approximation that is faster to compute than the exact value."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "But remember that Bayesian analysis is always based on modeling decisions, which implies that there is no âexactâ solution. For any interesting physical system there are many possible models, and each model yields different results. To interpret the results, we have to evaluate the models."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "So another interpretation of ABC is that it represents an alternative model of the likelihood. When we compute p(D|H), we are asking âWhat is the likelihood of the data under a given hypothesis?â"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "For large datasets, the likelihood of the data is very small, which is a hint that we might not be asking the right question. What we really want to know is the likelihood of any outcome like the data, where the definition of âlikeâ is yet another modeling decision."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The underlying idea of ABC is that two datasets are alike if they yield the same summary statistics. But in some cases, like the example in this chapter, it is not obvious which summary statistics to choose."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "You can download the code in this chapter from [http://thinkbayes.com/variability.py](http://thinkbayes.com/variability.py). For more information see [âWorking with the codeâ](preface01.html#download)."
        },
        {
          "cell_type": "heading",
          "level": 2,
          "metadata": {
          },
          "source": "Exercises"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<div id=\"a0000004849\" class=\"exercise\" data-type=\"example\">\n<h5></h5>\n<p>An âeffect sizeâ is a statistic intended to measure the\n        difference between two groups (see <a href=\"http://en.wikipedia.org/wiki/Effect_size\" class=\"orm:hideurl\">http://en.wikipedia.org/wiki/Effect_size</a>).</p>\n<p>For example, we could use data from the BRFSS to estimate the\n        difference in height between men and women. By sampling values from\n        the posterior distributions of <em>Î¼</em> and\n        <em>Ï</em>, we could generate the posterior\n        distribution of this difference.</p>\n<p>But it might be better to use a dimensionless measure of effect\n        size, rather than a difference measured in cm. One option is to use\n        divide through by the standard deviation (similar to what we did with\n        the coefficient of variation).</p>\n<p>If the parameters for Group 1 are <math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mo>(</mo><msub><mi>Î¼</mi><mn>1</mn></msub><mo>,</mo><msub><mi>Ï</mi><mn>1</mn></msub><mo>)</mo></mrow></math>, and the parameters for Group 1 are\n        <math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mo>(</mo><msub><mi>Î¼</mi><mn>2</mn></msub><mo>,</mo><msub><mi>Ï</mi><mn>2</mn></msub><mo>)</mo></mrow></math>, the dimensionless effect size is</p>\n<div data-type=\"equation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mfrac xmlns:mml=\"http://www.w3.org/1998/Math/MathML\"><mrow><msub><mi>Î¼</mi><mn>1</mn></msub><mo>-</mo><msub><mi>Î¼</mi><mn>2</mn></msub></mrow><mrow><mo>(</mo><msub><mi>Ï</mi><mn>1</mn></msub><mo>+</mo><msub><mi>Ï</mi><mn>2</mn></msub><mo>)</mo><mo>/</mo><mn>2</mn></mrow></mfrac></math></div>\n<p>Write a function that takes joint distributions of <code>mu</code> and <code>sigma</code> for two groups and returns the\n        posterior distribution of effect size.</p>\n<p>Hint: if enumerating all pairs from the two distributions takes\n        too long, consider random sampling.</p>\n</div>"
        }
      ],
      "metadata": {
      }
    }
  ]
}