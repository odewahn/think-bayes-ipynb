{
  "metadata": {
    "name": "Evidence"
  },
  "nbformat": 3,
  "nbformat_minor": 0,
  "worksheets": [
    {
      "cells": [
        {
          "cell_type": "heading",
          "level": 1,
          "metadata": {
          },
          "source": "Evidence"
        },
        {
          "cell_type": "heading",
          "level": 2,
          "metadata": {
          },
          "source": "Interpreting SAT scores"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Suppose you are the Dean of Admission at a small engineering college in Massachusetts, and you are considering two candidates, Alice and Bob, whose qualifications are similar in many ways, with the exception that Alice got a higher score on the Math portion of the SAT, a standardized test intended to measure preparation for college-level work in mathematics."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "If Alice got 780 and Bob got a 740 (out of a possible 800), you might want to know whether that difference is evidence that Alice is better prepared than Bob, and what the strength of that evidence is."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Now in reality, both scores are very good, and both candidates are probably well prepared for college math. So the real Dean of Admission would probably suggest that we choose the candidate who best demonstrates the other skills and attitudes we look for in students. But as an example of Bayesian hypothesis testing, letâs stick with a narrower question: âHow strong is the evidence that Alice is better prepared than Bob?â"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "To answer that question, we need to make some modeling decisions. Iâll start with a simplification I know is wrong; then weâll come back and improve the model. I pretend, temporarily, that all SAT questions are equally difficult. Actually, the designers of the SAT choose questions with a range of difficulty, because that improves the ability to measure statistical differences between test-takers."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "But if we choose a model where all questions are equally difficult, we can define a characteristic, `p_correct` , for each test-taker, which is the probability of answering any question correctly. This simplification makes it easy to compute the likelihood of a given score."
        },
        {
          "cell_type": "heading",
          "level": 2,
          "metadata": {
          },
          "source": "The scale"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "In order to understand SAT scores, we have to understand the scoring and scaling process. Each test-taker gets a raw score based on the number of correct and incorrect questions. The raw score is converted to a scaled score in the range 200â800."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "In 2009, there were 54 questions on the math SAT. The raw score for each test-taker is the number of questions answered correctly minus a penalty of 1/4 point for each question answered incorrectly."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The College Board, which administers the SAT, publishes the map from raw scores to scaled scores. I have downloaded that data and wrapped it in an Interpolator object that provides a forward lookup (from raw score to scaled) and a reverse lookup (from scaled score to raw)."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "You can download the code for this example from [http://thinkbayes.com/sat.py](http://thinkbayes.com/sat.py). For more information see [âWorking with the codeâ](preface01.html#download)."
        },
        {
          "cell_type": "heading",
          "level": 2,
          "metadata": {
          },
          "source": "The prior"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The College Board also publishes the distribution of scaled scores for all test-takers. If we convert each scaled score to a raw score, and divide by the number of questions, the result is an estimate of `p_correct` . So we can use the distribution of raw scores to model the prior distribution of `p_correct` ."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Here is the code that reads and processes the data:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "class Exam(object):\n\n    def __init__(self):\n        self.scale = ReadScale()\n        scores = ReadRanks()\n        score_pmf = thinkbayes.MakePmfFromDict(dict(scores))\n        self.raw = self.ReverseScale(score_pmf)\n        self.prior = DivideValues(raw, 54)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "`Exam` encapsulates the information we have about the exam. `ReadScale` and `ReadRanks` read files and return objects that contain the data: `self.scale` is the `Interpolator` that converts from raw to scaled scores and back; `scores` is a list of (score, frequency) pairs."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "`score_pmf` is the Pmf of scaled scores. `self.raw` is the Pmf of raw scores, and `self.prior` is the Pmf of `p_correct` ."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<figure id=\"fig.satprior\" style=\"float: True\"><img src=\"files/images/thba_1201.png\"><figcaption>Prior distribution of p_correct for SAT test-takers.</figcaption></figure>"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "FigureÂ 12-1 shows the prior distribution of `p_correct` . This distribution is approximately Gaussian, but it is compressed at the extremes. By design, the SAT has the most power to discriminate between test-takers within two standard deviations of the mean, and less power outside that range."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "For each test-taker, I define a Suite called `Sat` that represents the distribution of `p_correct` . Hereâs the definition:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "class Sat(thinkbayes.Suite):\n\n    def __init__(self, exam, score):\n        thinkbayes.Suite.__init__(self)\n\n        self.exam = exam\n        self.score = score\n\n        # start with the prior distribution\n        for p_correct, prob in exam.prior.Items():\n            self.Set(p_correct, prob)\n\n        # update based on an exam score\n        self.Update(score)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "`__init__` takes an Exam object and a scaled score. It makes a copy of the prior distribution and then updates itself based on the exam score."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "As usual, we inherit `Update` from `Suite` and provide `Likelihood` :"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "    def Likelihood(self, data, hypo):\n        p_correct = hypo\n        score = data\n\n        k = self.exam.Reverse(score)\n        n = self.exam.max_score\n        like = thinkbayes.EvalBinomialPmf(k, n, p_correct)\n        return like",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "`hypo` is a hypothetical value of `p_correct` , and `data` is a scaled score."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "To keep things simple, I interpret the raw score as the number of correct answers, ignoring the penalty for wrong answers. With this simplification, the likelihood is given by the binomial distribution, which computes the probability of _k_ correct responses out of _n_ questions."
        },
        {
          "cell_type": "heading",
          "level": 2,
          "metadata": {
          },
          "source": "Posterior"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "FigureÂ 12-2 shows the posterior distributions of `p_correct` for Alice and Bob based on their exam scores. We can see that they overlap, so it is possible that `p_correct` is actually higher for Bob, but it seems unlikely."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<figure id=\"fig.satposterior1\" style=\"float: none\"><img src=\"files/images/thba_1202.png\"><figcaption>Posterior distributions of p_correct for Alice and Bob.</figcaption></figure>"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Which brings us back to the original question, âHow strong is the evidence that Alice is better prepared than Bob?â We can use the posterior distributions of `p_correct` to answer this question."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "To formulate the question in terms of Bayesian hypothesis testing, I define two hypotheses:"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<ul>\n<li><p><em>A</em>: <code>p_correct</code> is higher for Alice than for\n        Bob.</p></li>\n<li><p><em>B</em>: <code>p_correct</code> is higher for Bob than for\n        Alice.</p></li>\n</ul>\n"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "To compute the likelihood of _A_, we can enumerate all pairs of values from the posterior distributions and add up the total probability of the cases where `p_correct` is higher for Alice than for Bob. And we already have a function, `thinkbayes.PmfProbGreater` , that does that."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "So we can define a Suite that computes the posterior probabilities of _A_ and _B_:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "class TopLevel(thinkbayes.Suite):\n\n    def Update(self, data):\n        a_sat, b_sat = data\n\n        a_like = thinkbayes.PmfProbGreater(a_sat, b_sat)\n        b_like = thinkbayes.PmfProbLess(a_sat, b_sat)\n        c_like = thinkbayes.PmfProbEqual(a_sat, b_sat)\n\n        a_like += c_like / 2\n        b_like += c_like / 2\n\n        self.Mult('A', a_like)\n        self.Mult('B', b_like)\n\n        self.Normalize()",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Usually when we define a new Suite, we inherit `Update` and provide `Likelihood` . In this case I override `Update` , because it is easier to evaluate the likelihood of both hypotheses at the same time."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The data passed to `Update` are Sat objects that represent the posterior distributions of `p_correct` ."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "`a_like` is the total probability that `p_correct` is higher for Alice; `b_like` is that probability that it is higher for Bob."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "`c_like` is the probability that they are âequal,â but this equality is an artifact of the decision to model `p_correct` with a set of discrete values. If we use more values, `c_like` is smaller, and in the extreme, if `p_correct` is continuous, `c_like` is zero. So I treat `c_like` as a kind of round-off error and split it evenly between `a_like` and `b_like` ."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Here is the code that creates `TopLevel` and updates it:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "    exam = Exam()\n    a_sat = Sat(exam, 780)\n    b_sat = Sat(exam, 740)\n\n    top = TopLevel('AB')\n    top.Update((a_sat, b_sat))\n    top.Print()",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The likelihood of _A_ is 0.79 and the likelihood of _B_ is 0.21. The likelihood ratio (or Bayes factor) is 3.8, which means that these test scores are evidence that Alice is better than Bob at answering SAT questions. If we believed, before seeing the test scores, that _A_ and _B_ were equally likely, then after seeing the scores we should believe that the probability of _A_ is 79%, which means there is still a 21% chance that Bob is actually better prepared."
        },
        {
          "cell_type": "heading",
          "level": 2,
          "metadata": {
          },
          "source": "A better model"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Remember that the analysis we have done so far is based on the simplification that all SAT questions are equally difficult. In reality, some are easier than others, which means that the difference between Alice and Bob might be even smaller."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "But how big is the modeling error? If it is small, we conclude that the first modelâbased on the simplification that all questions are equally difficultâis good enough. If itâs large, we need a better model."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "In the next few sections, I develop a better model and discover (spoiler alert!) that the modeling error is small. So if you are satisfied with the simple mode, you can skip to the next chapter. If you want to see how the more realistic model works, read on..."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<ul>\n<li><p>Assume that each test-taker has some degree of <code>efficacy</code>, which measures their ability to\n        answer SAT questions.<a data-type=\"indexterm\" data-primary=\"efficacy\" id=\"idp2868768\"></a></p></li>\n<li><p>Assume that each question has some level of <code>difficulty</code>.</p></li>\n<li>\n<p>Finally, assume that the chance that a test-taker answers a\n        question correctly is related to <code>efficacy</code> and <code>difficulty</code> according to this\n        function:</p>\n<pre data-type=\"programlisting\">def ProbCorrect(efficacy, difficulty, a=1):\n    return 1 / (1 + math.exp(-a * (efficacy - difficulty)))</pre>\n</li>\n</ul>\n"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "This function is a simplified version of the curve used in **item response theory**, which you can read about at [http://en.wikipedia.org/wiki/Item\\_response\\_theory](http://en.wikipedia.org/wiki/Item_response_theory). `efficacy` and `difficulty` are considered to be on the same scale, and the probability of getting a question right depends only on the difference between them."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "When `efficacy` and `difficulty` are equal, the probability of getting the question right is 50%. As `efficacy` increases, this probability approaches 100%. As it decreases (or as `difficulty` increases), the probability approaches 0%."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Given the distribution of `efficacy` across test-takers and the distribution of `difficulty` across questions, we can compute the expected distribution of raw scores. Weâll do that in two steps. First, for a person with given `efficacy` , weâll compute the distribution of raw scores."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "def PmfCorrect(efficacy, difficulties):\n    pmf0 = thinkbayes.Pmf([0])\n\n    ps = [ProbCorrect(efficacy, diff) for diff in difficulties]\n    pmfs = [BinaryPmf(p) for p in ps]\n    dist = sum(pmfs, pmf0)\n    return dist",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "`difficulties` is a list of difficulties, one for each question. `ps` is a list of probabilities, and `pmfs` is a list of two-valued Pmf objects; hereâs the function that makes them:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "def BinaryPmf(p):\n    pmf = thinkbayes.Pmf()\n    pmf.Set(1, p)\n    pmf.Set(0, 1-p)\n    return pmf",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "`dist` is the sum of these Pmfs. Remember from [âAddendsâ](ch05.html#addends) that when we add up Pmf objects, the result is the distribution of the sums. In order to use Pythonâs `sum` to add up Pmfs, we have to provide `pmf0` which is the identity for Pmfs, so `pmf + pmf0` is always `pmf` ."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "If we know a personâs efficacy, we can compute their distribution of raw scores. For a group of people with a different efficacies, the resulting distribution of raw scores is a mixture. Hereâs the code that computes the mixture:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "# class Exam:\n\n    def MakeRawScoreDist(self, efficacies):\n        pmfs = thinkbayes.Pmf()\n        for efficacy, prob in efficacies.Items():\n            scores = PmfCorrect(efficacy, self.difficulties)\n            pmfs.Set(scores, prob)\n\n        mix = thinkbayes.MakeMixture(pmfs)\n        return mix",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "`MakeRawScoreDist` takes `efficacies` , which is a Pmf that represents the distribution of efficacy across test-takers. I assume it is Gaussian with mean 0 and standard deviation 1.5. This choice is mostly arbitrary. The probability of getting a question correct depends on the difference between efficacy and difficulty, so we can choose the units of efficacy and then calibrate the units of difficulty accordingly."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "`pmfs` is a meta-Pmf that contains one Pmf for each level of efficacy, and maps to the fraction of test-takers at that level. `MakeMixture` takes the meta-pmf and computes the distribution of the mixture (see [âMixturesâ](ch05.html#mixture))."
        },
        {
          "cell_type": "heading",
          "level": 2,
          "metadata": {
          },
          "source": "Calibration"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "If we were given the distribution of difficulty, we could use `MakeRawScoreDist` to compute the distribution of raw scores. But for us the problem is the other way around: we are given the distribution of raw scores and we want to infer the distribution of difficulty."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "I assume that the distribution of difficulty is uniform with parameters `center` and `width` . `MakeDifficulties` makes a list of difficulties with these parameters."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "def MakeDifficulties(center, width, n):\n    low, high = center-width, center+width\n    return numpy.linspace(low, high, n)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "By trying out a few combinations, I found that `center=-0.05` and `width=1.8` yield a distribution of raw scores similar to the actual data, as shown in FigureÂ 12-3."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<figure id=\"fig.satcalibrate\" style=\"float: none\"><img src=\"files/images/thba_1203.png\"><figcaption>Actual distribution of raw scores and a model to fit it.</figcaption></figure>"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "So, assuming that the distribution of difficulty is uniform, its range is approximately `-1.85` to `1.75` , given that efficacy is Gaussian with mean 0 and standard deviation 1.5."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The following table shows the range of `ProbCorrect` for test-takers at different levels of efficacy:"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<table><tbody>\n<tr>\n<td></td>\n<td><p> Difficulty\n              </p></td>\n</tr>\n<tr>\n<td><p>Efficacy </p></td>\n<td><p> -1.85 </p></td>\n<td><p> -0.05 </p></td>\n<td><p> 1.75 </p></td>\n</tr>\n<tr>\n<td><p>3.00 </p></td>\n<td><p> 0.99 </p></td>\n<td><p> 0.95 </p></td>\n<td><p> 0.78 </p></td>\n</tr>\n<tr>\n<td><p>1.50 </p></td>\n<td><p> 0.97 </p></td>\n<td><p> 0.82 </p></td>\n<td><p> 0.44 </p></td>\n</tr>\n<tr>\n<td><p>0.00 </p></td>\n<td><p> 0.86 </p></td>\n<td><p> 0.51 </p></td>\n<td><p> 0.15 </p></td>\n</tr>\n<tr>\n<td><p>-1.50 </p></td>\n<td><p> 0.59 </p></td>\n<td><p> 0.19 </p></td>\n<td><p> 0.04 </p></td>\n</tr>\n<tr>\n<td><p>-3.00 </p></td>\n<td><p> 0.24 </p></td>\n<td><p> 0.05 </p></td>\n<td><p> 0.01 </p></td>\n</tr>\n</tbody></table>\n"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Someone with efficacy 3 (two standard deviations above the mean) has a 99% chance of answering the easiest questions on the exam, and a 78% chance of answering the hardest. On the other end of the range, someone two standard deviations below the mean has only a 24% chance of answering the easiest questions."
        },
        {
          "cell_type": "heading",
          "level": 2,
          "metadata": {
          },
          "source": "Posterior distribution of efficacy"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Now that the model is calibrated, we can compute the posterior distribution of efficacy for Alice and Bob. Here is a version of the Sat class that uses the new model:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "class Sat2(thinkbayes.Suite):\n\n    def __init__(self, exam, score):\n        self.exam = exam\n        self.score = score\n\n        # start with the Gaussian prior\n        efficacies = thinkbayes.MakeGaussianPmf(0, 1.5, 3)\n        thinkbayes.Suite.__init__(self, efficacies)\n\n        # update based on an exam score\n        self.Update(score)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "`Update` invokes `Likelihood` , which computes the likelihood of a given test score for a hypothetical level of efficacy."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "    def Likelihood(self, data, hypo):\n        efficacy = hypo\n        score = data\n        raw = self.exam.Reverse(score)\n\n        pmf = self.exam.PmfCorrect(efficacy)\n        like = pmf.Prob(raw)\n        return like",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "`pmf` is the distribution of raw scores for a test-taker with the given efficacy; `like` is the probability of the observed score."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "FigureÂ 12-4 shows the posterior distributions of efficacy for Alice and Bob. As expected, the location of Aliceâs distribution is farther to the right, but again there is some overlap."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<figure id=\"fig.satposterior2\" style=\"float: none\"><img src=\"files/images/thba_1204.png\"><figcaption>Posterior distributions of efficacy for Alice and Bob.</figcaption></figure>"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Using `TopLevel` again, we compare _A_, the hypothesis that Aliceâs efficacy is higher, and _B_, the hypothesis that Bobâs is higher. The likelihood ratio is 3.4, a bit smaller than what we got from the simple model (3.8). So this model indicates that the data are evidence in favor of _A_, but a little weaker than the previous estimate."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "If our prior belief is that _A_ and _B_ are equally likely, then in light of this evidence we would give _A_ a posterior probability of 77%, leaving a 23% chance that Bobâs efficacy is higher."
        },
        {
          "cell_type": "heading",
          "level": 2,
          "metadata": {
          },
          "source": "Predictive distribution"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The analysis we have done so far generates estimates for Alice and Bobâs efficacy, but since efficacy is not directly observable, it is hard to validate the results."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "To give the model predictive power, we can use it to answer a related question: âIf Alice and Bob take the math SAT again, what is the chance that Alice will do better again?â"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Weâll answer this question in two steps:"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<ul>\n<li><p>Weâll use the posterior distribution of efficacy to generate a\n        predictive distribution of raw score for each test-taker.</p></li>\n<li><p>Weâll compare the two predictive distributions to compute the\n        probability that Alice gets a higher score again.</p></li>\n</ul>\n"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "We already have most of the code we need. To compute the predictive distributions, we can use `MakeRawScoreDist` again:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "    exam = Exam()\n    a_sat = Sat(exam, 780)\n    b_sat = Sat(exam, 740)\n\n    a_pred = exam.MakeRawScoreDist(a_sat)\n    b_pred = exam.MakeRawScoreDist(b_sat)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Then we can find the likelihood that Alice does better on the second test, Bob does better, or they tie:"
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "    a_like = thinkbayes.PmfProbGreater(a_pred, b_pred)\n    b_like = thinkbayes.PmfProbLess(a_pred, b_pred)\n    c_like = thinkbayes.PmfProbEqual(a_pred, b_pred)",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The probability that Alice does better on the second exam is 63%, which means that Bob has a 37% chance of doing as well or better."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Notice that we have more confidence about Aliceâs efficacy than we do about the outcome of the next test. The posterior odds are 3:1 that Aliceâs efficacy is higher, but only 2:1 that Alice will do better on the next exam."
        },
        {
          "cell_type": "heading",
          "level": 2,
          "metadata": {
          },
          "source": "Discussion"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "We started this chapter with the question, âHow strong is the evidence that Alice is better prepared than Bob?â On the face of it, that sounds like we want to test two hypotheses: either Alice is more prepared or Bob is."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "But in order to compute likelihoods for these hypotheses, we have to solve an estimation problem. For each test-taker we have to find the posterior distribution of either `p_correct` or `efficacy` ."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "Values like this are called **nuisance parameters** because we donât care what they are, but we have to estimate them to answer the question we care about."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "One way to visualize the analysis we did in this chapter is to plot the space of these parameters. `thinkbayes.MakeJoint` takes two Pmfs, computes their joint distribution, and returns a joint pmf of each possible pair of values and its probability."
        },
        {
          "cell_type": "code",
          "collapsed": false,
          "input": "def MakeJoint(pmf1, pmf2):\n    joint = Joint()\n    for v1, p1 in pmf1.Items():\n        for v2, p2 in pmf2.Items():\n            joint.Set((v1, v2), p1 * p2)\n    return joint",
          "language": "python",
          "metadata": {
          },
          "outputs": [

          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "This function assumes that the two distributions are independent."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "FigureÂ 12-5 shows the joint posterior distribution of `p_correct` for Alice and Bob. The diagonal line indicates the part of the space where `p_correct` is the same for Alice and Bob. To the right of this line, Alice is more prepared; to the left, Bob is more prepared."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "<figure id=\"fig.satjoint\" style=\"float: none\"><img src=\"files/images/thba_1205.png\"><figcaption>Joint posterior distribution of p_correct for Alice and\n      Bob.</figcaption></figure>"
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "In `TopLevel.Update` , when we compute the likelihoods of _A_ and _B_, we add up the probability mass on each side of this line. For the cells that fall on the line, we add up the total mass and split it between _A_ and _B_."
        },
        {
          "cell_type": "markdown",
          "metadata": {
          },
          "source": "The process we used in this chapterâestimating nuisance parameters in order to evaluate the likelihood of competing hypothesesâis a common Bayesian approach to problems like this."
        }
      ],
      "metadata": {
      }
    }
  ]
}